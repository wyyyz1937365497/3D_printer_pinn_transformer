# visualize_physical_predictor.py
# ç‰©ç†é¢„æµ‹æ¨¡å‹æ¨ç†æ•ˆæœå¯è§†åŒ–è„šæœ¬

import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
import os
import pickle
from train_physical_predictor import EnhancedPhysicalPredictor, Config

def load_model_and_data(checkpoint_path, data_path, config):
    """åŠ è½½æ¨¡å‹å’Œæ•°æ®"""
    print("  æ­£åœ¨åŠ è½½æ¨¡å‹...")
    # åŠ è½½æ¨¡å‹
    model = EnhancedPhysicalPredictor(config)
    
    checkpoint = torch.load(checkpoint_path, map_location=config.device)
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯DataParallelæ¨¡å‹
    state_dict = checkpoint['model_state_dict']
    if any(key.startswith('module.') for key in state_dict.keys()):
        # å¦‚æœæ˜¯DataParallelä¿å­˜çš„æ¨¡å‹ï¼Œå»æ‰module.å‰ç¼€
        new_state_dict = {}
        for key, value in state_dict.items():
            new_key = key[7:] if key.startswith('module.') else key  # å»æ‰'module.'å‰ç¼€
            new_state_dict[new_key] = value
        state_dict = new_state_dict
    
    model.load_state_dict(state_dict)
    
    if len(config.gpu_ids) > 1:
        model = nn.DataParallel(model, device_ids=config.gpu_ids)
    model = model.to(config.device)
    model.eval()
    
    print("  æ­£åœ¨åŠ è½½æ ‡å‡†åŒ–å‚æ•°...")
    # åŠ è½½æ ‡å‡†åŒ–å‚æ•°
    norm_path = os.path.join(os.path.dirname(checkpoint_path), 'normalization_params.pkl')
    with open(norm_path, 'rb') as f:
        norm_params = pickle.load(f)
    
    print("  æ­£åœ¨åŠ è½½æ•°æ®...")
    # åŠ è½½æ•°æ®
    df = pd.read_csv(data_path)
    
    return model, df, norm_params

def preprocess_data(df, config, norm_params):
    """é¢„å¤„ç†æ•°æ®"""
    print("  æ­£åœ¨é¢„å¤„ç†æ•°æ®...")
    # é€‰æ‹©ç‰¹å¾åˆ—
    feature_cols = config.feature_cols
    target_cols = config.target_cols
    
    # æå–ç‰¹å¾å’Œç›®æ ‡
    features = df[feature_cols].values.astype(np.float32)
    targets = df[target_cols].values.astype(np.float32)
    
    # è®¡ç®—é¢‘åŸŸç‰¹å¾
    freq_features = compute_frequency_features(features, config)
    
    # åˆå¹¶æ—¶åŸŸå’Œé¢‘åŸŸç‰¹å¾
    combined_features = np.concatenate([features, freq_features], axis=1)
    
    # æ£€æŸ¥æ ‡å‡†åŒ–å‚æ•°ç»´åº¦æ˜¯å¦åŒ¹é…
    expected_feature_dim = norm_params['feature_mean'].shape[0]
    actual_feature_dim = combined_features.shape[1]
    
    if expected_feature_dim != actual_feature_dim:
        print(f"  è­¦å‘Š: æ ‡å‡†åŒ–å‚æ•°ç»´åº¦ä¸åŒ¹é… - æœŸæœ›: {expected_feature_dim}, å®é™…: {actual_feature_dim}")
        print("  æ­£åœ¨é‡æ–°è®¡ç®—æ ‡å‡†åŒ–å‚æ•°...")
        
        # ä½¿ç”¨å½“å‰æ•°æ®çš„ç»Ÿè®¡é‡è¿›è¡Œæ ‡å‡†åŒ–
        feature_mean = np.mean(combined_features, axis=0)
        feature_std = np.std(combined_features, axis=0) + 1e-8
    else:
        # ä½¿ç”¨ä¿å­˜çš„æ ‡å‡†åŒ–å‚æ•°
        feature_mean = norm_params['feature_mean']
        feature_std = norm_params['feature_std']
    
    combined_features = (combined_features - feature_mean) / feature_std
    
    return combined_features, targets

def compute_frequency_features(features, config):
    """è®¡ç®—é¢‘åŸŸç‰¹å¾"""
    # ä½¿ç”¨æ»‘åŠ¨çª—å£è®¡ç®—é¢‘åŸŸç‰¹å¾
    window_size = min(config.seq_len, features.shape[0])
    stride = config.pred_len
    
    freq_features_list = []
    
    for i in range(0, max(1, features.shape[0] - window_size + 1), stride):
        window = features[i:i+window_size]
        
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è®¡ç®—é¢‘åŸŸè¡¨ç¤º
        freq_data = []
        for j in range(window.shape[1]):
            # ä½¿ç”¨FFTè®¡ç®—é¢‘åŸŸç‰¹å¾
            fft_vals = np.fft.fft(window[:, j])
            fft_magnitude = np.abs(fft_vals[:config.freq_bands])  # å–å‰freq_bandsä¸ªé¢‘ç‡æˆåˆ†
            freq_data.extend(fft_magnitude)
        
        freq_features_list.append(freq_data)
    
    # å¦‚æœåºåˆ—é•¿åº¦ä¸å¤Ÿä¸€ä¸ªçª—å£ï¼Œå¤åˆ¶æœ€åä¸€ä¸ªçª—å£çš„æ•°æ®
    if len(freq_features_list) == 0:
        # è®¡ç®—æ•´ä¸ªåºåˆ—çš„é¢‘åŸŸç‰¹å¾
        freq_data = []
        for j in range(features.shape[1]):
            fft_vals = np.fft.fft(features[:, j])
            fft_magnitude = np.abs(fft_vals[:config.freq_bands])
            freq_data.extend(fft_magnitude)
        freq_features_list = [freq_data] * features.shape[0]
    elif len(freq_features_list) < features.shape[0]:
        # æ‰©å±•é¢‘åŸŸç‰¹å¾ä»¥åŒ¹é…åŸå§‹åºåˆ—é•¿åº¦
        last_freq_features = freq_features_list[-1]
        while len(freq_features_list) < features.shape[0]:
            freq_features_list.append(last_freq_features)
    
    return np.array(freq_features_list, dtype=np.float32)

def predict_with_model(model, features, config):
    """ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    print("  æ­£åœ¨è¿›è¡Œæ¨¡å‹é¢„æµ‹...")
    model.eval()
    
    with torch.no_grad():
        # é‡å¡‘æ•°æ®ä¸ºåºåˆ—æ ¼å¼
        seq_len = config.seq_len
        n_samples = (len(features) // seq_len) * seq_len  # è°ƒæ•´ä¸ºèƒ½è¢«seq_lenæ•´é™¤çš„é•¿åº¦
        features = features[:n_samples]
        
        # é‡å¡‘ä¸º (batch, seq_len, features)
        features = features.reshape(-1, seq_len, features.shape[-1])
        
        # åªå–å‰10ä¸ªåºåˆ—è¿›è¡Œé¢„æµ‹ï¼Œé¿å…é•¿æ—¶é—´è¿è¡Œ
        features = features[:10]
        
        # è½¬æ¢ä¸ºtensor
        features_tensor = torch.FloatTensor(features).to(config.device)
        
        # åˆ†æ‰¹é¢„æµ‹ä»¥é¿å…å†…å­˜é—®é¢˜
        batch_size = min(config.batch_size, len(features_tensor))
        predictions = []
        
        for i in range(0, len(features_tensor), batch_size):
            batch = features_tensor[i:i+batch_size]
            
            with torch.cuda.amp.autocast():
                pred = model(batch)
            
            predictions.append(pred.cpu().numpy())
    
    # åˆå¹¶é¢„æµ‹ç»“æœ
    predictions = np.vstack(predictions)
    
    # é‡å¡‘å›åŸå§‹å½¢çŠ¶
    predictions = predictions.reshape(-1, predictions.shape[-1])
    
    return predictions

def denormalize_data(data, norm_params, target_cols):
    """åæ ‡å‡†åŒ–æ•°æ®"""
    print("  æ­£åœ¨åæ ‡å‡†åŒ–æ•°æ®...")
    target_mean = norm_params['target_mean']
    target_std = norm_params['target_std']
    
    # è·å–ç›®æ ‡åˆ—çš„ç´¢å¼•
    target_idx = [i for i, col in enumerate(norm_params['target_cols']) if col in target_cols]
    
    if len(target_idx) != len(target_cols):
        # å¦‚æœç›®æ ‡åˆ—ä¸å®Œå…¨åŒ¹é…ï¼Œä½¿ç”¨å…¨éƒ¨ç›®æ ‡åˆ—
        target_idx = list(range(len(target_cols)))
    
    if len(target_idx) > 0:
        target_mean = target_mean[target_idx]
        target_std = target_std[target_idx]
    
    denorm_data = data * (target_std + 1e-8) + target_mean
    
    return denorm_data

def plot_predictions_vs_actual(y_true, y_pred, target_cols, title="ç‰©ç†é¢„æµ‹æ¨¡å‹æ•ˆæœ"):
    """ç»˜åˆ¶é¢„æµ‹å€¼ä¸çœŸå®å€¼å¯¹æ¯”å›¾"""
    print("  æ­£åœ¨ç”Ÿæˆé¢„æµ‹å€¼ä¸çœŸå®å€¼å¯¹æ¯”å›¾...")
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'Liberation Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    n_targets = min(len(target_cols), y_pred.shape[1])
    
    fig, axes = plt.subplots(n_targets, 1, figsize=(12, 4*n_targets))
    if n_targets == 1:
        axes = [axes]
    
    for i in range(n_targets):
        axes[i].scatter(y_true[:, i], y_pred[:, i], alpha=0.5, s=1)
        axes[i].plot([y_true[:, i].min(), y_true[:, i].max()], 
                    [y_true[:, i].min(), y_true[:, i].max()], 'r--', lw=2)
        axes[i].set_xlabel(f'çœŸå®å€¼ - {target_cols[i]}')
        axes[i].set_ylabel(f'é¢„æµ‹å€¼ - {target_cols[i]}')
        axes[i].set_title(f'{target_cols[i]}: é¢„æµ‹å€¼ vs çœŸå®å€¼')
        axes[i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join('./checkpoints_physical_predictor_enhanced', f'{title.replace(" ", "_")}.png'), 
                dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()  # å…³é—­å›¾å½¢ä»¥é‡Šæ”¾å†…å­˜
    print(f"  é¢„æµ‹å€¼ä¸çœŸå®å€¼å¯¹æ¯”å›¾å·²ä¿å­˜è‡³: {os.path.join('./checkpoints_physical_predictor_enhanced', f'{title.replace(' ', '_')}.png')}")

def plot_time_series(y_true, y_pred, target_cols, start_idx=0, end_idx=1000, title="æ—¶é—´åºåˆ—é¢„æµ‹"):
    """ç»˜åˆ¶æ—¶é—´åºåˆ—é¢„æµ‹ç»“æœ"""
    print("  æ­£åœ¨ç”Ÿæˆæ—¶é—´åºåˆ—é¢„æµ‹å›¾...")
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'Liberation Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    n_targets = min(len(target_cols), y_pred.shape[1])
    
    fig, axes = plt.subplots(n_targets, 1, figsize=(15, 3*n_targets))
    if n_targets == 1:
        axes = [axes]
    
    for i in range(n_targets):
        axes[i].plot(y_true[start_idx:end_idx, i], label='çœŸå®å€¼', alpha=0.7)
        axes[i].plot(y_pred[start_idx:end_idx, i], label='é¢„æµ‹å€¼', alpha=0.7)
        axes[i].set_xlabel('æ—¶é—´æ­¥')
        axes[i].set_ylabel(target_cols[i])
        axes[i].set_title(f'{target_cols[i]} æ—¶é—´åºåˆ—é¢„æµ‹æ•ˆæœ')
        axes[i].legend()
        axes[i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join('./checkpoints_physical_predictor_enhanced', f'{title.replace(" ", "_")}_timeseries.png'), 
                dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()  # å…³é—­å›¾å½¢ä»¥é‡Šæ”¾å†…å­˜
    print(f"  æ—¶é—´åºåˆ—é¢„æµ‹å›¾å·²ä¿å­˜è‡³: {os.path.join('./checkpoints_physical_predictor_enhanced', f'{title.replace(' ', '_')}_timeseries.png')}")

def calculate_metrics(y_true, y_pred):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    print("  æ­£åœ¨è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    # MSE
    mse = np.mean((y_true - y_pred) ** 2, axis=0)
    # MAE
    mae = np.mean(np.abs(y_true - y_pred), axis=0)
    # RMSE
    rmse = np.sqrt(mse)
    # RÂ²
    ss_res = np.sum((y_true - y_pred) ** 2, axis=0)
    ss_tot = np.sum((y_true - np.mean(y_true, axis=0)) ** 2, axis=0)
    r2 = 1 - (ss_res / ss_tot)
    
    return {'MSE': mse, 'MAE': mae, 'RMSE': rmse, 'RÂ²': r2}

def main():
    print("="*80)
    print("ğŸ”¬ ç‰©ç†é¢„æµ‹æ¨¡å‹æ¨ç†æ•ˆæœå¯è§†åŒ–")
    print("="*80)
    
    # é…ç½®å‚æ•°
    config = Config()
    config.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
    checkpoint_path = os.path.join(config.checkpoint_dir, 'best_physical_predictor.pth')
    if not os.path.exists(checkpoint_path):
        print(f"âŒ æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_path}")
        # å°è¯•ä½¿ç”¨æœ€æ–°çš„æ£€æŸ¥ç‚¹
        checkpoint_files = [f for f in os.listdir(config.checkpoint_dir) if f.endswith('.pth') and 'best' not in f]
        if checkpoint_files:
            checkpoint_files.sort(key=lambda x: os.path.getmtime(os.path.join(config.checkpoint_dir, x)), reverse=True)
            checkpoint_path = os.path.join(config.checkpoint_dir, checkpoint_files[0])
            print(f"âœ… ä½¿ç”¨æœ€æ–°æ£€æŸ¥ç‚¹: {checkpoint_path}")
        else:
            print(f"âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹æ£€æŸ¥ç‚¹æ–‡ä»¶")
            return
    
    # é€‰æ‹©æµ‹è¯•æ•°æ®æ–‡ä»¶
    data_dir = config.data_dir
    data_files = [f for f in os.listdir(data_dir) if f.startswith('machine_') and f.endswith('.csv')]
    if not data_files:
        print(f"âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        return
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°æ®æ–‡ä»¶è¿›è¡Œå¯è§†åŒ–
    data_path = os.path.join(data_dir, data_files[0])
    print(f"ğŸ“Š ä½¿ç”¨æ•°æ®æ–‡ä»¶: {data_files[0]}")
    
    # åŠ è½½æ¨¡å‹å’Œæ•°æ®
    print("ğŸ”„ åŠ è½½æ¨¡å‹å’Œæ•°æ®...")
    model, df, norm_params = load_model_and_data(checkpoint_path, data_path, config)
    
    print(f"   æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"   ç‰¹å¾åˆ—: {config.feature_cols}")
    print(f"   ç›®æ ‡åˆ—: {config.target_cols}")
    
    # é¢„å¤„ç†æ•°æ®
    features, targets = preprocess_data(df, config, norm_params)
    print(f"   é¢„å¤„ç†åç‰¹å¾å½¢çŠ¶: {features.shape}")
    print(f"   é¢„å¤„ç†åç›®æ ‡å½¢çŠ¶: {targets.shape}")
    
    # å‡†å¤‡ç”¨äºé¢„æµ‹çš„æ•°æ® - ä¸é¢„æµ‹å‡½æ•°ä¸­ç›¸åŒçš„å¤„ç†
    seq_len = config.seq_len
    n_samples = (len(features) // seq_len) * seq_len
    targets_for_pred = targets[:n_samples]
    
    # é‡å¡‘ä¸ºåºåˆ—æ ¼å¼ï¼Œç„¶åæå–ç”¨äºé¢„æµ‹çš„åºåˆ—
    targets_for_pred = targets_for_pred.reshape(-1, seq_len, targets_for_pred.shape[-1])
    
    # åªå–å‰10ä¸ªåºåˆ—ç”¨äºé¢„æµ‹ï¼ˆä¸é¢„æµ‹å‡½æ•°ä¸€è‡´ï¼‰
    targets_for_pred = targets_for_pred[:10]
    
    # é‡å¡‘å›åŸå§‹å½¢çŠ¶ - ç°åœ¨æ˜¯ (10 * seq_len, features)ï¼Œå³ (2500, 5)
    targets_for_pred = targets_for_pred.reshape(-1, targets_for_pred.shape[-1])
    
    # æ¨¡å‹é¢„æµ‹
    predictions = predict_with_model(model, features, config)
    print(f"   é¢„æµ‹ç»“æœå½¢çŠ¶: {predictions.shape}")
    
    # åæ ‡å‡†åŒ–
    targets_denorm = denormalize_data(targets_for_pred, norm_params, config.target_cols)
    predictions_denorm = denormalize_data(predictions, norm_params, config.target_cols)
    print("   æ•°æ®åæ ‡å‡†åŒ–å®Œæˆ")
    
    # ç¡®ä¿ä¸¤ä¸ªæ•°ç»„å½¢çŠ¶ä¸€è‡´
    print(f"   çœŸå®å€¼å½¢çŠ¶: {targets_denorm.shape}")
    print(f"   é¢„æµ‹å€¼å½¢çŠ¶: {predictions_denorm.shape}")
    
    if targets_denorm.shape != predictions_denorm.shape:
        print(f"âš ï¸  å½¢çŠ¶ä¸ä¸€è‡´ï¼Œè°ƒæ•´åˆ°ç›¸åŒå½¢çŠ¶")
        min_len = min(len(targets_denorm), len(predictions_denorm))
        targets_denorm = targets_denorm[:min_len]
        predictions_denorm = predictions_denorm[:min_len]
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    metrics = calculate_metrics(targets_denorm, predictions_denorm)
    
    print("\nğŸ“Š è¯„ä¼°æŒ‡æ ‡:")
    for metric_name, values in metrics.items():
        print(f"  {metric_name}:")
        for i, col in enumerate(config.target_cols[:len(values)]):
            print(f"    {col}: {values[i]:.6f}")
    
    # ç»˜åˆ¶å¯è§†åŒ–å›¾è¡¨
    print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    # é¢„æµ‹å€¼ä¸çœŸå®å€¼å¯¹æ¯”
    plot_predictions_vs_actual(targets_denorm, predictions_denorm, config.target_cols)
    
    # æ—¶é—´åºåˆ—é¢„æµ‹æ•ˆæœï¼ˆå‰1000ä¸ªç‚¹ï¼‰
    plot_time_series(targets_denorm, predictions_denorm, config.target_cols, 
                    start_idx=0, end_idx=min(1000, len(targets_denorm)))
    
    # å¦‚æœæ•°æ®é‡å¤§ï¼Œä¹Ÿå¯ä»¥ç»˜åˆ¶åé¢çš„ç‰‡æ®µ
    if len(targets_denorm) > 2000:
        plot_time_series(targets_denorm, predictions_denorm, config.target_cols,
                        start_idx=len(targets_denorm)//2, 
                        end_idx=min(len(targets_denorm)//2 + 1000, len(targets_denorm)),
                        title="æ—¶é—´åºåˆ—é¢„æµ‹(ååŠæ®µ)")
    
    print(f"\nğŸ‰ å¯è§†åŒ–å®Œæˆï¼å›¾è¡¨å·²ä¿å­˜è‡³: {config.checkpoint_dir}")

if __name__ == "__main__":
    # å®šä¹‰é…ç½®ç±»ï¼Œä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
    class Config:
        def __init__(self):
            self.checkpoint_dir = './checkpoints_physical_predictor_enhanced'
            self.data_dir = './printer_dataset_correction'
            self.seq_len = 250
            self.pred_len = 50
            self.batch_size = 1024
            self.gradient_accumulation_steps = 1
            self.model_dim = 192  # ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
            self.num_heads = 8
            self.num_layers = 6  # ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
            self.dim_feedforward = 768  # å‰é¦ˆç½‘ç»œç»´åº¦
            self.dropout = 0.1
            self.lr = 5e-5
            self.epochs = 30
            self.gpu_ids = [0]
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.lambda_physics = 0.4
            self.lambda_freq = 0.3
            self.warmup_epochs = 5
            
            # ç‰¹å¾å’Œç›®æ ‡åˆ—
            self.feature_cols = [
                'nozzle_x', 'nozzle_y', 'nozzle_z',  # æ›¿æ¢åŸæ¥çš„æ§åˆ¶åˆ—
                'temperature_C', 'vibration_disp_x_m', 'vibration_disp_y_m',
                'vibration_vel_x_m_s', 'vibration_vel_y_m_s',
                'motor_current_x_A', 'motor_current_y_A',
                'pressure_bar'
            ]
            self.target_cols = [
                'vibration_disp_x_m', 'vibration_disp_y_m',
                'temperature_C', 'motor_current_x_A', 'motor_current_y_A'
            ]
            self.freq_bands = 8
            self.sampling_rate = 1000
            self.max_freq = 500
            self.time_domain_dim = len(self.feature_cols)
            self.freq_domain_dim = self.freq_bands * len(self.feature_cols)
            self.input_dim = self.time_domain_dim + self.freq_domain_dim  # å®é™…è¾“å…¥ç»´åº¦
            self.output_dim = len(self.target_cols)
            
            # æ·»åŠ å…¶ä»–å¿…è¦å‚æ•°
            self.n_heads = self.num_heads  # ä¸num_headsç›¸åŒ
            self.temperature = 1.0  # æ³¨æ„åŠ›æ¸©åº¦
            self.layer_norm_eps = 1e-5  # å±‚å½’ä¸€åŒ–epsilon
            self.max_len = 5000  # æœ€å¤§åºåˆ—é•¿åº¦
            self.num_freq_components = 10  # é¢‘åŸŸç»„ä»¶æ•°é‡
    
    # è°ƒç”¨ä¸»å‡½æ•°
    main()