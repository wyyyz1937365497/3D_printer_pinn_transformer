# train_fault_classifier.py
# ç‹¬ç«‹çš„æ•…éšœåˆ†ç±»å™¨ï¼Œä½¿ç”¨Transformerç¼–ç å™¨æå–ç‰¹å¾
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
import os
import time
import pickle
from sklearn.model_selection import train_test_split
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
import argparse
from datetime import datetime, timedelta

# ==================== é…ç½®å‚æ•° ====================
class Config:
    def __init__(self, resume_from=None, gpu_ids=[0]):
        self.data_path = 'printer_dataset_correction/printer_gear_correction_dataset.csv'
        self.batch_size = 256
        self.lr = 1e-4
        self.epochs = 25
        self.gpu_ids = gpu_ids
        self.resume_from = resume_from  # æ·»åŠ ç»§ç»­è®­ç»ƒçš„è·¯å¾„
        if len(gpu_ids) > 1:
            self.device = f'cuda:{gpu_ids[0]}'  # ä¸»GPU
        else:
            self.device = f'cuda:{gpu_ids[0]}' if torch.cuda.is_available() else 'cpu'
        self.checkpoint_dir = './checkpoints_fault_classifier'
        self.max_samples = 30000
        self.seq_len = 150  # é•¿åºåˆ—ï¼Œæ•è·æ•…éšœæ¨¡å¼
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # ç‰¹å¾åˆ—
        self.feature_cols = [
            'temperature_C', 'vibration_disp_x_m', 'vibration_disp_y_m',
            'vibration_vel_x_m_s', 'vibration_vel_y_m_s',
            'motor_current_x_A', 'motor_current_y_A',
            'pressure_bar'
        ]
        
        # æ•…éšœç±»å‹: 0=æ­£å¸¸, 1=å–·å˜´å µå¡, 2=æœºæ¢°æ¾åŠ¨, 3=ç”µæœºæ•…éšœ
        self.n_classes = 4
        self.input_dim = len(self.feature_cols)
        self.model_dim = 128

# ==================== æ•…éšœåˆ†ç±»æ¨¡å‹ ====================
class FaultClassifier(nn.Module):
    def __init__(self, config):
        super(FaultClassifier, self).__init__()
        self.embedding = nn.Linear(config.input_dim, config.model_dim)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=config.model_dim,
            nhead=8,
            dim_feedforward=512,
            dropout=0.1,
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)
        
        self.classifier = nn.Sequential(
            nn.Linear(config.model_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, config.n_classes)
        )
    
    def forward(self, x):
        # x: [batch, seq_len, input_dim]
        x = self.embedding(x)  # [batch, seq_len, model_dim]
        memory = self.encoder(x)  # [batch, seq_len, model_dim]
        
        # ä½¿ç”¨åºåˆ—æ‰€æœ‰æ—¶é—´æ­¥çš„å¹³å‡å€¼
        seq_avg = torch.mean(memory, dim=1)  # [batch, model_dim]
        
        return self.classifier(seq_avg)

# ==================== æ•°æ®é›†ç±» ====================
class FaultDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = torch.tensor(sequences, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.long)
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        return self.sequences[idx], self.labels[idx]

# ==================== æ•°æ®å¤„ç†å™¨ ====================
def prepare_fault_data(config):
    print("ğŸ”„ åŠ è½½æ•…éšœæ•°æ®...")
    df = pd.read_csv(config.data_path)
    
    # ç‰¹å¾å’Œæ ‡ç­¾
    features = df[config.feature_cols].values
    labels = df['fault_label'].values
    
    # æ ‡å‡†åŒ–
    feature_mean = features.mean(axis=0)
    feature_std = features.std(axis=0)
    feature_std[feature_std < 1e-8] = 1.0
    features_norm = (features - feature_mean) / feature_std
    
    # åˆ›å»ºåºåˆ—æ ·æœ¬
    sequences = []
    sequence_labels = []
    
    machine_ids = df['machine_id'].unique()
    
    for mid in machine_ids:
        machine_data = df[df['machine_id'] == mid]
        machine_features = features_norm[df['machine_id'] == mid]
        machine_labels = labels[df['machine_id'] == mid]
        
        # æŒ‰seq_lené•¿åº¦åˆ‡åˆ†åºåˆ—
        for i in range(0, len(machine_data) - config.seq_len, config.seq_len):
            seq = machine_features[i:i+config.seq_len]
            # ä½¿ç”¨åºåˆ—æœ€åä¸€ä¸ªä½ç½®çš„æ ‡ç­¾
            label = machine_labels[i+config.seq_len-1]
            
            if len(seq) == config.seq_len:  # ç¡®ä¿åºåˆ—é•¿åº¦æ­£ç¡®
                sequences.append(seq)
                sequence_labels.append(label)
    
    sequences = np.array(sequences)
    sequence_labels = np.array(sequence_labels)
    
    # é™åˆ¶æ ·æœ¬æ•°é‡
    if len(sequences) > config.max_samples:
        idx = np.random.choice(len(sequences), config.max_samples, replace=False)
        sequences = sequences[idx]
        sequence_labels = sequence_labels[idx]
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_seq, val_seq, train_labels, val_labels = train_test_split(
        sequences, sequence_labels, test_size=0.2, random_state=42
    )
    
    print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(sequences)}")
    print(f"   è®­ç»ƒé›†: {len(train_seq)}, éªŒè¯é›†: {len(val_seq)}")
    print(f"   æ•…éšœåˆ†å¸ƒ: {np.bincount(sequence_labels)}")
    
    return (train_seq, train_labels), (val_seq, val_labels)

# ==================== è®­ç»ƒå‡½æ•° ====================
def train_fault_classifier(config):
    print("=" * 80)
    print("ğŸš€ è®­ç»ƒæ•…éšœåˆ†ç±»å™¨")
    print("=" * 80)
    
    # å‡†å¤‡æ•°æ®
    (train_seq, train_labels), (val_seq, val_labels) = prepare_fault_data(config)
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = FaultDataset(train_seq, train_labels)
    val_dataset = FaultDataset(val_seq, val_labels)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = FaultClassifier(config)
    print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ | å‚æ•°é‡: {sum(p.numel() for p in model.parameters())}")
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¤šGPU
    if len(config.gpu_ids) > 1:
        print(f"âœ… ä½¿ç”¨å¤šGPUè®­ç»ƒ: {config.gpu_ids}")
        model = nn.DataParallel(model, device_ids=config.gpu_ids)
        model = model.to(config.device)
    else:
        model = model.to(config.device)
    
    # ä¼˜åŒ–å™¨
    optimizer = AdamW(model.parameters(), lr=config.lr, weight_decay=1e-4)
    scheduler = ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3
    )
    
    # æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()
    
    # ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    start_epoch = 0
    best_val_loss = float('inf')
    train_losses = []
    val_losses = []
    
    if config.resume_from and os.path.exists(config.resume_from):
        print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {config.resume_from}")
        checkpoint = torch.load(config.resume_from)
        if isinstance(model, nn.DataParallel):
            model.module.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch']
        best_val_loss = checkpoint['best_val_loss']
        train_losses = checkpoint.get('train_losses', [])
        val_losses = checkpoint.get('val_losses', [])
        print(f"âœ… æ¢å¤è®­ç»ƒæˆåŠŸ | ä»ç¬¬ {start_epoch} ä¸ªepochå¼€å§‹")
    
    print("\nğŸ”¥ å¼€å§‹è®­ç»ƒæ•…éšœåˆ†ç±»å™¨...")
    print("-" * 80)
    
    for epoch in range(start_epoch, config.epochs):
        epoch_start = time.time()
        model.train()
        total_loss = 0
        
        for batch_idx, (seq, label) in enumerate(train_loader):
            seq, label = seq.to(config.device), label.to(config.device)
            
            pred = model(seq)
            loss = criterion(pred, label)
            
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            total_loss += loss.item()
        
        avg_train_loss = total_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # éªŒè¯
        model.eval()
        val_loss = 0
        
        with torch.no_grad():
            for seq, label in val_loader:
                seq, label = seq.to(config.device), label.to(config.device)
                pred = model(seq)
                loss = criterion(pred, label)
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        
        scheduler.step(avg_val_loss)
        epoch_time = time.time() - epoch_start
        
        # è®¡ç®—å‰©ä½™æ—¶é—´
        elapsed_time = time.time() - epoch_start
        remaining_epochs = config.epochs - epoch - 1
        remaining_time = elapsed_time * remaining_epochs
        remaining_time_str = str(timedelta(seconds=int(remaining_time)))
        
        print(f"âœ… Epoch {epoch+1:2d}/{config.epochs} | "
              f"Train Loss: {avg_train_loss:.6f} | "
              f"Val Loss: {avg_val_loss:.6f} | "
              f"Time: {epoch_time:.2f}s | "
              f"å‰©ä½™æ—¶é—´: {remaining_time_str}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            checkpoint_data = {
                'epoch': epoch + 1,
                'model_state_dict': model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'best_val_loss': best_val_loss,
                'train_losses': train_losses,
                'val_losses': val_losses,
                'config': config.__dict__
            }
            torch.save(checkpoint_data, os.path.join(config.checkpoint_dir, 'best_fault_classifier.pth'))
            print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ•…éšœåˆ†ç±»å™¨ (éªŒè¯æŸå¤±: {best_val_loss:.6f})")
    
    print("\n" + "=" * 80)
    print(f"ğŸ‰ æ•…éšœåˆ†ç±»å™¨è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print("=" * 80)

# ==================== ä¸»å‡½æ•° ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='è®­ç»ƒæ•…éšœåˆ†ç±»å™¨')
    parser.add_argument('--resume', type=str, default=None, help='ä»æŒ‡å®šè·¯å¾„æ¢å¤è®­ç»ƒ')
    parser.add_argument('--gpu_ids', type=str, default='0,1', help='GPU IDs (ä¾‹å¦‚: "0,1,2,3")')
    args = parser.parse_args()
    
    gpu_ids = [int(id) for id in args.gpu_ids.split(',')]
    config = Config(resume_from=args.resume, gpu_ids=gpu_ids)
    train_fault_classifier(config)