# train_correction_controller_streaming.py
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from torch.utils.data import IterableDataset, DataLoader
import os
import time
import pickle
import matplotlib.pyplot as plt
from torch.optim import AdamW
from torch.amp import autocast, GradScaler
import argparse
from datetime import datetime, timedelta

plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class Config:
    def __init__(self, resume_from=None, gpu_ids=[0]):
        self.data_dir = 'printer_dataset_correction/'
        self.batch_size = 1024  # å¢åŠ batch size
        self.lr = 3e-4
        self.epochs = 30  # è°ƒæ•´epochæ•°
        self.gpu_ids = gpu_ids
        self.resume_from = resume_from
        self.device = f'cuda:{gpu_ids[0]}' if torch.cuda.is_available() else 'cpu'
        self.checkpoint_dir = './checkpoints_correction_controller'  # ä¿®æ”¹æ£€æŸ¥ç‚¹ç›®å½•
        self.seq_len = 50
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        # ä¿®æ”¹ç‰¹å¾åˆ—ï¼Œä½¿ç”¨å®é™…æ•°æ®é›†ä¸­çš„åˆ—å
        self.feature_cols = [
            'nozzle_x', 'nozzle_y', 'nozzle_z',
            'temperature_C', 'vibration_disp_x_m', 'vibration_disp_y_m',
            'vibration_vel_x_m_s', 'vibration_vel_y_m_s',
            'motor_current_x_A', 'motor_current_y_A',
            'pressure_bar'
        ]
        self.correction_cols = ['correction_x_mm', 'correction_y_mm', 'correction_temp_C']
        self.input_dim = len(self.feature_cols)
        self.output_dim = len(self.correction_cols)

class CorrectionController(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(config.input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, config.output_dim)
        )
    
    def forward(self, x):
        return self.net(x)

class StreamingCorrectionDataset(IterableDataset):
    def __init__(self, data_dir, config, split='train', val_ratio=0.2, norm_params=None):
        self.data_dir = data_dir
        self.config = config
        self.split = split
        self.val_ratio = val_ratio
        self.files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) 
                           if f.startswith('machine_') and f.endswith('.csv')])
        
        # åŠ è½½æ ‡å‡†åŒ–å‚æ•°
        if norm_params is None:
            self._load_norm_params()
        else:
            self.feature_mean = norm_params['feature_mean']
            self.feature_std = norm_params['feature_std']
            self.correction_mean = norm_params.get('correction_mean', np.zeros(len(config.correction_cols)))
            self.correction_std = norm_params.get('correction_std', np.ones(len(config.correction_cols)))
    
    def _load_norm_params(self):
        path = './checkpoints_physical_predictor_enhanced/normalization_params.pkl'  # ä¿®æ”¹ä¸ºæ­£ç¡®çš„è·¯å¾„
        if os.path.exists(path):
            with open(path, 'rb') as f:
                params = pickle.load(f)
                self.feature_mean = params['feature_mean']
                self.feature_std = params['feature_std']
        else:
            raise FileNotFoundError("è¯·å…ˆè®­ç»ƒç‰©ç†é¢„æµ‹æ¨¡å‹ä»¥è·å–æ ‡å‡†åŒ–å‚æ•°")
        
        # å°è¯•åŠ è½½çŸ«æ­£å‚æ•°
        correction_path = './checkpoints_correction_controller/correction_params.pkl'
        if os.path.exists(correction_path):
            with open(correction_path, 'rb') as f:
                corr_params = pickle.load(f)
                self.correction_mean = corr_params['correction_mean']
                self.correction_std = corr_params['correction_std']
        else:
            # ä¼°ç®—
            self.correction_mean = np.array([0.0, 0.0, 0.0])
            self.correction_std = np.array([0.01, 0.01, 10.0])
    
    def _process_file(self, filepath):
        df = pd.read_csv(filepath)
        normal_df = df[df['fault_label'] == 0]
        # åªä¿ç•™æŒ¯åŠ¨å¹…åº¦å¤§äºé˜ˆå€¼çš„æ ·æœ¬ï¼ˆéœ€è¦çŸ«æ­£çš„åŒºåŸŸï¼‰
        mask = (normal_df['vibration_disp_x_m'].abs() + normal_df['vibration_disp_y_m'].abs()) > 0.0005
        normal_df = normal_df[mask]
        
        if len(normal_df) == 0:
            return
        
        features = normal_df[self.config.feature_cols].values
        corrections = normal_df[self.config.correction_cols].values
        
        features = (features - self.feature_mean) / self.feature_std
        corrections = (corrections - self.correction_mean) / self.correction_std
        
        indices = np.arange(len(features))
        val_size = int(len(indices) * self.val_ratio)
        
        if self.split == 'train':
            indices = indices[:-val_size] if val_size > 0 else indices
        else:
            indices = indices[-val_size:] if val_size > 0 else []
        
        for i in indices:
            yield torch.from_numpy(features[i].astype(np.float32)), torch.from_numpy(corrections[i].astype(np.float32))
    
    def __iter__(self):
        worker_info = torch.utils.data.get_worker_info()
        if worker_info is None:
            for f in self.files:
                yield from self._process_file(f)
        else:
            worker_id = worker_info.id
            num_workers = worker_info.num_workers
            files_per_worker = len(self.files) // num_workers
            start_idx = worker_id * files_per_worker
            end_idx = start_idx + files_per_worker if worker_id < num_workers - 1 else len(self.files)
            
            for f in self.files[start_idx:end_idx]:
                yield from self._process_file(f)

def train_correction_controller(config):
    print("=" * 80)
    print("ğŸš€ è®­ç»ƒæµå¼çŸ«æ­£æ§åˆ¶å™¨")
    print("=" * 80)
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = StreamingCorrectionDataset(config.data_dir, config, split='train')
    val_dataset = StreamingCorrectionDataset(config.data_dir, config, split='val')
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        num_workers=4,
        pin_memory=True,
        prefetch_factor=2
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        num_workers=2,
        pin_memory=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = CorrectionController(config)
    if len(config.gpu_ids) > 1:
        model = nn.DataParallel(model, device_ids=config.gpu_ids)
    model = model.to(config.device)
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=0.01)
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler()
    
    # è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    train_losses = []
    val_losses = []
    
    # æ·»åŠ æå‰åœæ­¢ç›¸å…³å‚æ•°
    patience = 5  # å…è®¸è¿ç»­5ä¸ªepochéªŒè¯æŸå¤±ä¸ä¸‹é™ååœæ­¢è®­ç»ƒ
    patience_counter = 0  # è®¡æ•°å™¨
    min_delta = 0.001  # éªŒè¯æŸå¤±éœ€è¦ä¸‹é™çš„æœ€å°å€¼
    
    for epoch in range(config.epochs):
        model.train()
        total_loss = 0
        num_batches = 0
        
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config.epochs}")
        for batch_idx, (features, corrections) in enumerate(train_pbar):
            features, corrections = features.to(config.device, non_blocking=True), corrections.to(config.device, non_blocking=True)
            
            with autocast(device_type='cuda'):
                outputs = model(features)
                loss = nn.MSELoss()(outputs, corrections)
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaNæˆ–æ— ç©·å¤§
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"âš ï¸  è·³è¿‡æ‰¹æ¬¡ {batch_idx}ï¼Œæ£€æµ‹åˆ°æ— æ•ˆæŸå¤±å€¼")
                continue
            
            scaler.scale(loss).backward()
            
            if (batch_idx + 1) % config.gradient_accumulation_steps == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
            
            total_loss += loss.item()
            num_batches += 1
            
            train_pbar.set_postfix({'Loss': f"{loss.item():.6f}"})
        
        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
        train_losses.append(avg_loss)
        
        # éªŒè¯
        val_loss = validate_correction_controller(model, val_loader, config, scaler)
        val_losses.append(val_loss)
        
        print(f"Epoch {epoch+1}/{config.epochs} | Train Loss: {avg_loss:.6f} | Val Loss: {val_loss:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_loss:
            best_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_loss,
            }, os.path.join(config.checkpoint_dir, 'best_correction_controller.pth'))
            print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (Loss: {best_loss:.6f})")
            patience_counter = 0  # é‡ç½®è®¡æ•°å™¨
        else:
            patience_counter += 1
            
        # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
        if (epoch + 1) % 5 == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': val_loss,
            }, os.path.join(config.checkpoint_dir, f'correction_controller_epoch{epoch+1}.pth'))
            print(f"ğŸ’¾ epoch {epoch+1} çš„æ£€æŸ¥ç‚¹å·²ä¿å­˜")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æå‰åœæ­¢
        if patience_counter >= patience:
            print(f"âš ï¸  éªŒè¯æŸå¤±è¿ç»­ {patience} ä¸ªepochæœªæ”¹å–„ï¼Œåœæ­¢è®­ç»ƒ...")
            break
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.title('æ ¡æ­£æ§åˆ¶å™¨è®­ç»ƒæ›²çº¿')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(os.path.join(config.checkpoint_dir, 'correction_controller_training_curve.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ‰ æ ¡æ­£æ§åˆ¶å™¨è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_loss:.6f}")

# ==================== ä¸»å‡½æ•° ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='è®­ç»ƒæµå¼çŸ«æ­£æ§åˆ¶å™¨')
    parser.add_argument('--resume', type=str, default=None, help='ä»æŒ‡å®šè·¯å¾„æ¢å¤è®­ç»ƒ')
    parser.add_argument('--gpu_ids', type=str, default='0,1', help='GPU IDs (ä¾‹å¦‚: "0,1,2,3")')
    args = parser.parse_args()
    gpu_ids = [int(id) for id in args.gpu_ids.split(',')]
    
    config = Config(resume_from=args.resume, gpu_ids=gpu_ids)
    train_correction_controller(config)