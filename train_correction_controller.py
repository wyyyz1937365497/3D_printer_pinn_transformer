# train_correction_controller_streaming.py
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from torch.utils.data import IterableDataset, DataLoader
import os
import time
import pickle
import matplotlib.pyplot as plt
from torch.optim import AdamW
from torch.amp import autocast, GradScaler
import argparse
from datetime import datetime, timedelta

plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class Config:
    def __init__(self, resume_from=None, gpu_ids=[0]):
        self.data_dir = 'printer_dataset_correction/'
        self.batch_size = 1024  # å¢åŠ batch size
        self.lr = 3e-4
        self.epochs = 30  # è°ƒæ•´epochæ•°
        self.gpu_ids = gpu_ids
        self.resume_from = resume_from
        self.device = f'cuda:{gpu_ids[0]}' if torch.cuda.is_available() else 'cpu'
        self.checkpoint_dir = './checkpoints_correction_controller'  # ä¿®æ”¹æ£€æŸ¥ç‚¹ç›®å½•
        self.seq_len = 50
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        # ä¿®æ”¹ç‰¹å¾åˆ—ï¼Œä½¿ç”¨å®é™…æ•°æ®é›†ä¸­çš„åˆ—å
        self.feature_cols = [
            'nozzle_x', 'nozzle_y', 'nozzle_z',
            'temperature_C', 'vibration_disp_x_m', 'vibration_disp_y_m',
            'vibration_vel_x_m_s', 'vibration_vel_y_m_s',
            'motor_current_x_A', 'motor_current_y_A',
            'pressure_bar'
        ]
        self.correction_cols = ['correction_x_mm', 'correction_y_mm', 'correction_temp_C']
        self.input_dim = len(self.feature_cols)
        self.output_dim = len(self.correction_cols)

class CorrectionController(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(config.input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, config.output_dim)
        )
    
    def forward(self, x):
        return self.net(x)

class StreamingCorrectionDataset(IterableDataset):
    def __init__(self, data_dir, config, split='train', val_ratio=0.2, norm_params=None):
        self.data_dir = data_dir
        self.config = config
        self.split = split
        self.val_ratio = val_ratio
        self.files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) 
                           if f.startswith('machine_') and f.endswith('.csv')])
        
        # åŠ è½½æ ‡å‡†åŒ–å‚æ•°
        if norm_params is None:
            self._load_norm_params()
        else:
            self.feature_mean = norm_params['feature_mean']
            self.feature_std = norm_params['feature_std']
            self.correction_mean = norm_params.get('correction_mean', np.zeros(len(config.correction_cols)))
            self.correction_std = norm_params.get('correction_std', np.ones(len(config.correction_cols)))
    
    def _load_norm_params(self):
        path = './checkpoints_physical_predictor_enhanced/normalization_params.pkl'  # ä¿®æ”¹ä¸ºæ­£ç¡®çš„è·¯å¾„
        if os.path.exists(path):
            with open(path, 'rb') as f:
                params = pickle.load(f)
                self.feature_mean = params['feature_mean']
                self.feature_std = params['feature_std']
        else:
            raise FileNotFoundError("è¯·å…ˆè®­ç»ƒç‰©ç†é¢„æµ‹æ¨¡å‹ä»¥è·å–æ ‡å‡†åŒ–å‚æ•°")
        
        # å°è¯•åŠ è½½çŸ«æ­£å‚æ•°
        correction_path = './checkpoints_correction_controller/correction_params.pkl'
        if os.path.exists(correction_path):
            with open(correction_path, 'rb') as f:
                corr_params = pickle.load(f)
                self.correction_mean = corr_params['correction_mean']
                self.correction_std = corr_params['correction_std']
        else:
            # ä¼°ç®—
            self.correction_mean = np.array([0.0, 0.0, 0.0])
            self.correction_std = np.array([0.01, 0.01, 10.0])
    
    def _process_file(self, filepath):
        df = pd.read_csv(filepath)
        normal_df = df[df['fault_label'] == 0]
        # åªä¿ç•™æŒ¯åŠ¨å¹…åº¦å¤§äºé˜ˆå€¼çš„æ ·æœ¬ï¼ˆéœ€è¦çŸ«æ­£çš„åŒºåŸŸï¼‰
        mask = (normal_df['vibration_disp_x_m'].abs() + normal_df['vibration_disp_y_m'].abs()) > 0.0005
        normal_df = normal_df[mask]
        
        if len(normal_df) == 0:
            return
        
        features = normal_df[self.config.feature_cols].values
        corrections = normal_df[self.config.correction_cols].values
        
        features = (features - self.feature_mean) / self.feature_std
        corrections = (corrections - self.correction_mean) / self.correction_std
        
        indices = np.arange(len(features))
        val_size = int(len(indices) * self.val_ratio)
        
        if self.split == 'train':
            indices = indices[:-val_size] if val_size > 0 else indices
        else:
            indices = indices[-val_size:] if val_size > 0 else []
        
        for i in indices:
            yield torch.from_numpy(features[i].astype(np.float32)), torch.from_numpy(corrections[i].astype(np.float32))
    
    def __iter__(self):
        worker_info = torch.utils.data.get_worker_info()
        if worker_info is None:
            for f in self.files:
                yield from self._process_file(f)
        else:
            worker_id = worker_info.id
            num_workers = worker_info.num_workers
            files_per_worker = len(self.files) // num_workers
            start_idx = worker_id * files_per_worker
            end_idx = start_idx + files_per_worker if worker_id < num_workers - 1 else len(self.files)
            
            for f in self.files[start_idx:end_idx]:
                yield from self._process_file(f)

def train_correction_controller(config):
    print("=" * 80)
    print("ğŸš€ è®­ç»ƒæµå¼çŸ«æ­£æ§åˆ¶å™¨")
    print("=" * 80)
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = StreamingCorrectionDataset(config.data_dir, config, split='train')
    val_dataset = StreamingCorrectionDataset(config.data_dir, config, split='val')
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        num_workers=4,
        pin_memory=True,
        prefetch_factor=2
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        num_workers=2,
        pin_memory=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = CorrectionController(config).to(config.device)
    if len(config.gpu_ids) > 1:
        model = nn.DataParallel(model, device_ids=config.gpu_ids)
    
    print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ | å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # ä¼˜åŒ–å™¨
    optimizer = AdamW(model.parameters(), lr=config.lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3
    )
    
    criterion = nn.MSELoss()
    scaler = GradScaler('cuda')
    
    # æ¢å¤è®­ç»ƒ
    start_epoch = 0
    best_val_loss = float('inf')
    train_losses = []
    val_losses = []
    
    if config.resume_from and os.path.exists(config.resume_from):
        print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {config.resume_from}")
        checkpoint = torch.load(config.resume_from, map_location=config.device)
        model_state = checkpoint['model_state_dict']
        
        if isinstance(model, nn.DataParallel):
            model.module.load_state_dict(model_state)
        else:
            model.load_state_dict(model_state)
        
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch']
        best_val_loss = checkpoint['best_val_loss']
        train_losses = checkpoint.get('train_losses', [])
        val_losses = checkpoint.get('val_losses', [])
        print(f"âœ… æ¢å¤è®­ç»ƒæˆåŠŸ | ä»ç¬¬ {start_epoch} ä¸ªepochå¼€å§‹")
    
    print("\nğŸ”¥ å¼€å§‹è®­ç»ƒçŸ«æ­£æ§åˆ¶å™¨...")
    print("-" * 80)
    
    for epoch in range(start_epoch, config.epochs):
        epoch_start = time.time()
        model.train()
        total_loss = 0
        
        for batch_idx, (feat, corr) in enumerate(train_loader):
            feat, corr = feat.to(config.device), corr.to(config.device)
            
            with autocast('cuda'):
                pred = model(feat)
                loss = criterion(pred, corr)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            
            total_loss += loss.item()
        
        avg_train_loss = total_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # éªŒè¯
        model.eval()
        val_loss = 0
        
        with torch.no_grad():
            for feat, corr in val_loader:
                feat, corr = feat.to(config.device), corr.to(config.device)
                pred = model(feat)
                loss = criterion(pred, corr)
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        scheduler.step(avg_val_loss)
        
        epoch_time = time.time() - epoch_start
        elapsed_time = time.time() - epoch_start
        remaining_epochs = config.epochs - epoch - 1
        remaining_time = elapsed_time * remaining_epochs
        remaining_time_str = str(timedelta(seconds=int(remaining_time)))
        
        print(f"âœ… Epoch {epoch+1:2d}/{config.epochs} | "
              f"Train Loss: {avg_train_loss:.6f} | "
              f"Val Loss: {avg_val_loss:.6f} | "
              f"Time: {epoch_time:.2f}s | "
              f"å‰©ä½™æ—¶é—´: {remaining_time_str}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            checkpoint_data = {
                'epoch': epoch + 1,
                'model_state_dict': model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'best_val_loss': best_val_loss,
                'train_losses': train_losses,
                'val_losses': val_losses,
                'config': config.__dict__
            }
            torch.save(checkpoint_data, os.path.join(config.checkpoint_dir, 'best_correction_controller.pth'))
            print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³çŸ«æ­£æ§åˆ¶å™¨ (éªŒè¯æŸå¤±: {best_val_loss:.6f})")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='è®­ç»ƒæŸå¤±')
    plt.plot(val_losses, label='éªŒè¯æŸå¤±')
    plt.xlabel('Epoch')
    plt.ylabel('æŸå¤±')
    plt.title('çŸ«æ­£æ§åˆ¶å™¨è®­ç»ƒè¿‡ç¨‹')
    plt.legend()
    plt.grid(True)
    plt.savefig(os.path.join(config.checkpoint_dir, 'correction_training_curve.png'))
    
    print("\n" + "=" * 80)
    print(f"ğŸ‰ çŸ«æ­£æ§åˆ¶å™¨è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print("=" * 80)

# ==================== ä¸»å‡½æ•° ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='è®­ç»ƒæµå¼çŸ«æ­£æ§åˆ¶å™¨')
    parser.add_argument('--resume', type=str, default=None, help='ä»æŒ‡å®šè·¯å¾„æ¢å¤è®­ç»ƒ')
    parser.add_argument('--gpu_ids', type=str, default='0,1', help='GPU IDs (ä¾‹å¦‚: "0,1,2,3")')
    args = parser.parse_args()
    gpu_ids = [int(id) for id in args.gpu_ids.split(',')]
    
    config = Config(resume_from=args.resume, gpu_ids=gpu_ids)
    train_correction_controller(config)