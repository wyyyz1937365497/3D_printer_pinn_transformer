# train_pinn_transformer_multitask.py
import pickle
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
import os
import time
import gc
import argparse
from torch.amp import autocast, GradScaler
from torch.utils.tensorboard import SummaryWriter
from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import signal
import atexit
import warnings
warnings.filterwarnings('ignore')
import os
import torch

# Windowså¤šGPUä¼˜åŒ–é…ç½®
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"  # æŒ‡å®šä½¿ç”¨ä¸¤å¼ å¡
os.environ["NCCL_P2P_DISABLE"] = "1"         # Windowsä¸‹NCCLä¼˜åŒ–
os.environ["CUDA_LAUNCH_BLOCKING"] = "0"     # éé˜»å¡æ¨¡å¼

# æ£€æŸ¥GPUå¯ç”¨æ€§
# print(f"å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
# for i in range(torch.cuda.device_count()):
#     print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
#     print(f"  æ˜¾å­˜: {torch.cuda.get_device_properties(i).total_memory/1024**3:.1f}GB")


# ==================== é…ç½®å‚æ•° ====================
class Config:
    def __init__(self):
        self.data_path = 'printer_dataset/nozzle_simulation_gear_print.csv'
        self.seq_len = 200          # å†å²çª—å£é•¿åº¦ï¼ˆ200ms @ 1msæ­¥é•¿ï¼‰
        self.pred_len = 50          # é¢„æµ‹é•¿åº¦ï¼ˆ50msï¼‰
        self.batch_size = 1024
        self.gradient_accumulation_steps = 2
        self.model_dim = 256
        self.num_heads = 8
        self.num_layers = 6
        self.dim_feedforward = 1024
        self.dropout = 0.1
        self.lr = 1e-4
        self.epochs = 50
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.num_workers = 4
        self.max_samples = 500000
        self.lambda_physics = 0.1   # ç‰©ç†æŸå¤±æƒé‡
        self.lambda_classification = 1.0  # åˆ†ç±»æŸå¤±æƒé‡
        self.lambda_rul = 0.5       # RULæŸå¤±æƒé‡
        self.warmup_epochs = 5
        self.checkpoint_dir = './checkpoints_multitask'
        self.resume_from = None
        self.save_on_exit = True
        self.save_interval = 5
        self.start_epoch = 0
        self.load_optimizer_state = True
        self.pin_memory = True
        # åˆ—å®šä¹‰
        self.ctrl_cols = ['ctrl_T_target', 'ctrl_speed_set', 'ctrl_pos_x', 'ctrl_pos_y', 'ctrl_pos_z']
        self.state_cols = ['temperature_C', 'vibration_disp_x_m', 'vibration_disp_y_m', 
                          'vibration_vel_x_m_s', 'vibration_vel_y_m_s', 
                          'motor_current_x_A', 'motor_current_y_A', 'motor_current_z_A',
                          'pressure_bar', 'nozzle_pos_x_mm', 'nozzle_pos_y_mm', 'nozzle_pos_z_mm',
                          'print_quality']
        self.target_cols = ['fault_label', 'fault_type', 'print_quality']
        
        # ç»´åº¦å®šä¹‰
        self.input_dim = len(self.ctrl_cols) + len(self.state_cols) + 1  # +1 for hour feature
        self.output_dim = len(self.state_cols)
        self.ctrl_dim = len(self.ctrl_cols)
        self.class_dim = 4  # 3ç§æ•…éšœç±»å‹ + æ­£å¸¸
        self.rul_dim = 1    # RULé¢„æµ‹

# ==================== ä½ç½®ç¼–ç  ====================
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:, :x.size(1), :]

# ==================== å¤šä»»åŠ¡PINN-Transformeræ¨¡å‹ ====================
class PrinterPINN_MultiTask(nn.Module):
    def __init__(self, config):
        super(PrinterPINN_MultiTask, self).__init__()
        self.input_dim = config.input_dim
        self.output_dim = config.output_dim
        self.ctrl_dim = config.ctrl_dim
        self.d_model = config.model_dim
        self.pred_len = config.pred_len
        self.class_dim = config.class_dim
        self.rul_dim = config.rul_dim
        
        # å…±äº«ç¼–ç å™¨
        self.encoder_embedding = nn.Linear(self.input_dim, self.d_model)
        self.pos_encoder = PositionalEncoding(self.d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.d_model,
            nhead=config.num_heads,
            dim_feedforward=config.dim_feedforward,
            dropout=config.dropout,
            batch_first=True,
            norm_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)
        
        # è§£ç å™¨ï¼ˆç”¨äºç‰©ç†åœºé‡æ„ï¼‰
        self.decoder_embedding = nn.Linear(self.ctrl_dim, self.d_model)
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=self.d_model,
            nhead=config.num_heads,
            dim_feedforward=config.dim_feedforward,
            dropout=config.dropout,
            batch_first=True,
            norm_first=True
        )
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=config.num_layers)
        self.fc_out = nn.Linear(self.d_model, self.output_dim)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(self.d_model, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, self.class_dim)
        )
        
        # RULå›å½’å¤´
        self.rul_predictor = nn.Sequential(
            nn.Linear(self.d_model, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, self.rul_dim),
            nn.ReLU()  # RULåº”ä¸ºæ­£æ•°
        )
        
    def forward(self, src, tgt_ctrl):
        # å…±äº«ç¼–ç å™¨
        src_emb = self.encoder_embedding(src)
        src_emb = self.pos_encoder(src_emb)
        memory = self.encoder(src_emb)  # [batch, seq_len, d_model]
        
        # ç‰©ç†åœºé‡æ„è§£ç å™¨
        tgt_emb = self.decoder_embedding(tgt_ctrl)
        tgt_emb = self.pos_encoder(tgt_emb)
        decoder_output = self.decoder(tgt_emb, memory)
        physics_pred = self.fc_out(decoder_output)  # [batch, pred_len, output_dim]
        
        # ä½¿ç”¨ç¼–ç å™¨çš„æœ€ç»ˆçŠ¶æ€è¿›è¡Œåˆ†ç±»å’ŒRULé¢„æµ‹
        # å–åºåˆ—çš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        last_hidden = memory[:, -1, :]  # [batch, d_model]
        
        # æ•…éšœåˆ†ç±»
        class_pred = self.classifier(last_hidden)  # [batch, class_dim]
        
        # RULé¢„æµ‹
        rul_pred = self.rul_predictor(last_hidden)  # [batch, 1]
        
        return {
            'physics_pred': physics_pred,
            'class_pred': class_pred,
            'rul_pred': rul_pred,
            'memory': memory
        }
    
    def physics_loss(self, outputs, targets, device='cuda'):
        """ç‰©ç†çº¦æŸæŸå¤±ï¼ˆé’ˆå¯¹3Dæ‰“å°å–·å¤´åŠ¨åŠ›å­¦ï¼‰"""
        physics_pred = outputs['physics_pred']
        y_true = targets
        
        loss = 0.0
        batch_size, seq_len, _ = physics_pred.shape
        
        # 1. çƒ­ä¼ å¯¼æ–¹ç¨‹çº¦æŸï¼ˆæ¸©åº¦å˜åŒ–åº”å¹³æ»‘ï¼‰
        temp_pred = physics_pred[:, :, 0]  # temperature_C
        if seq_len > 1:
            dT_dt = torch.diff(temp_pred, dim=1) / 0.001  # 1msæ­¥é•¿
            if dT_dt.shape[1] > 1:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å…ƒç´ è¿›è¡ŒäºŒæ¬¡å¾®åˆ†
                d2T_dt2 = torch.diff(dT_dt, dim=1) / 0.001
                # æ¸©åº¦åŠ é€Ÿåº¦åº”æœ‰é™ï¼ˆé¿å…ä¸åˆç†çš„å‰§çƒˆå˜åŒ–ï¼‰
                thermal_loss = torch.mean(torch.abs(d2T_dt2))
                # æ·»åŠ é˜ˆå€¼é˜²æ­¢æ— ç©·å¤§
                thermal_loss = torch.clamp(thermal_loss, max=1e3)
                loss += thermal_loss
        
        # 2. æŒ¯åŠ¨åŠ¨åŠ›å­¦çº¦æŸï¼ˆè´¨é‡-å¼¹ç°§-é˜»å°¼ç³»ç»Ÿï¼‰
        if seq_len > 1:
            disp_x_pred = physics_pred[:, :, 1]  # vibration_disp_x_m
            disp_y_pred = physics_pred[:, :, 2]  # vibration_disp_y_m
            vel_x_pred = physics_pred[:, :, 3]   # vibration_vel_x_m_s
            vel_y_pred = physics_pred[:, :, 4]   # vibration_vel_y_m_s
            
            # ä»ä½ç§»è®¡ç®—é€Ÿåº¦ï¼ˆåº”ä¸é¢„æµ‹çš„é€Ÿåº¦ä¸€è‡´ï¼‰
            dt = 0.001  # 1ms
            if disp_x_pred.shape[1] > 1 and disp_y_pred.shape[1] > 1:
                vel_x_from_disp = torch.diff(disp_x_pred, dim=1) / dt
                vel_y_from_disp = torch.diff(disp_y_pred, dim=1) / dt
                
                # é€Ÿåº¦ä¸€è‡´æ€§æŸå¤±
                vibration_loss = torch.mean((vel_x_from_disp - vel_x_pred[:, :-1])**2) + \
                                torch.mean((vel_y_from_disp - vel_y_pred[:, :-1])**2)
                # æ·»åŠ é˜ˆå€¼é˜²æ­¢æ— ç©·å¤§
                vibration_loss = torch.clamp(vibration_loss, max=1e3)
                loss += vibration_loss
        
        # 3. èƒ½é‡å®ˆæ’çº¦æŸï¼ˆç®€åŒ–çš„ï¼‰
        if seq_len > 1:
            vel_x_pred = physics_pred[:, :, 3]   # vibration_vel_x_m_s
            vel_y_pred = physics_pred[:, :, 4]   # vibration_vel_y_m_s
            kinetic_energy = vel_x_pred**2 + vel_y_pred**2
            if kinetic_energy.shape[1] > 1:
                d_energy_dt = torch.diff(kinetic_energy, dim=1) / dt
                energy_loss = torch.mean(torch.abs(d_energy_dt))
                # æ·»åŠ é˜ˆå€¼é˜²æ­¢æ— ç©·å¤§
                energy_loss = torch.clamp(energy_loss, max=1e2)
                loss += 0.1 * energy_loss
        
        # 4. ç”µæœºç”µæµ-æŒ¯åŠ¨è€¦åˆçº¦æŸ
        if seq_len > 1:
            current_x_pred = physics_pred[:, :, 5]  # motor_current_x_A
            current_y_pred = physics_pred[:, :, 6]  # motor_current_y_A
            
            # ç”µæµåº”ä¸åŠ é€Ÿåº¦ç›¸å…³ï¼ˆF=maï¼Œè€ŒFä¸ç”µæµæˆæ­£æ¯”ï¼‰
            dt = 0.001  # 1ms
            if vel_x_pred.shape[1] > 1 and vel_y_pred.shape[1] > 1:
                accel_x_pred = torch.diff(vel_x_pred, dim=1) / dt
                accel_y_pred = torch.diff(vel_y_pred, dim=1) / dt
                
                if accel_x_pred.shape[1] > 0 and accel_y_pred.shape[1] > 0:
                    current_accel_corr_x = torch.mean(current_x_pred[:, :-1] * accel_x_pred)
                    current_accel_corr_y = torch.mean(current_y_pred[:, :-1] * accel_y_pred)
                    
                    # ç¡®ä¿ç›¸å…³æ€§åˆç†ï¼ˆé¿å…å®Œå…¨ä¸ç›¸å…³çš„é¢„æµ‹ï¼‰
                    coupling_loss = torch.abs(1.0 - torch.abs(current_accel_corr_x)) + \
                                   torch.abs(1.0 - torch.abs(current_accel_corr_y))
                    # æ·»åŠ é˜ˆå€¼é˜²æ­¢æ— ç©·å¤§
                    coupling_loss = torch.clamp(coupling_loss, max=1e2)
                    loss += 0.2 * coupling_loss
        
        return loss

# ==================== æ•°æ®å¤„ç†å™¨ ====================
class MultiTaskDataProcessor:
    def __init__(self, data_path, seq_len, pred_len, max_samples, config):
        self.data_path = data_path
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.max_samples = max_samples
        self.config = config
        
        print(f"ğŸ”„ å¼€å§‹å¤„ç†å¤šä»»åŠ¡æ•°æ®...")
        print(f"ğŸ“Š å†å²é•¿åº¦: {seq_len}, é¢„æµ‹é•¿åº¦: {pred_len}")
        self.process_data()
    
    def process_data(self):
        """å¤„ç†æ•°æ®ç”¨äºå¤šä»»åŠ¡è®­ç»ƒ"""
        df = pd.read_csv(self.data_path)
        print(f"âœ… åŸå§‹æ•°æ®åŠ è½½: {df.shape}")
        
        # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
        numeric_cols = self.config.ctrl_cols + self.config.state_cols + ['fault_label', 'fault_type', 'timestamp']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = df[col].astype(np.float32)
        
        # ç‰¹å¾å·¥ç¨‹ï¼šæ·»åŠ æ—¶é—´ç‰¹å¾
        df['hour'] = (df['timestamp'] % 3600) / 3600  # å°æ—¶å‘¨æœŸ
        
        # é€‰æ‹©ç›¸å…³åˆ—
        all_cols = self.config.ctrl_cols + self.config.state_cols + ['hour']
        target_cols = ['fault_label', 'fault_type', 'print_quality']
        
        grouped = df.groupby('machine_id')
        samples = []
        count = 0
        
        print("ğŸ“Š æ”¶é›†æ ·æœ¬ç´¢å¼•...")
        for machine_id, group in grouped:
            group = group.sort_values('timestamp').reset_index(drop=True)
            
            # æ•°æ®æ•°ç»„
            data_array = group[all_cols].values
            ctrl_array = group[self.config.ctrl_cols].values
            target_array = group[target_cols].values
            
            total_len = len(group)
            required_len = self.seq_len + self.pred_len
            
            if total_len < required_len:
                continue
            
            n_windows = total_len - required_len + 1
            
            for i in range(n_windows):
                if count >= self.max_samples:
                    break
                
                # æ£€æŸ¥çª—å£å†…æ˜¯å¦æœ‰æ•…éšœ
                window_fault = target_array[i:i+required_len, 0]  # fault_label
                
                # å¦‚æœçª—å£å†…æœ‰æ•…éšœï¼Œåªåœ¨æ•…éšœå‘ç”Ÿåçš„çª—å£ä½¿ç”¨
                if np.any(window_fault == 1):
                    fault_indices = np.where(window_fault == 1)[0]
                    first_fault_idx = fault_indices[0]
                    if first_fault_idx < self.seq_len:  # æ•…éšœåœ¨å†å²çª—å£å†…
                        continue
                
                # æå–æ ·æœ¬
                x_hist = data_array[i:i+self.seq_len]
                x_future_ctrl = ctrl_array[i+self.seq_len:i+required_len]
                y_future_state = data_array[i+self.seq_len:i+required_len, len(self.config.ctrl_cols):len(self.config.ctrl_cols)+len(self.config.state_cols)]
                y_targets = target_array[i+self.seq_len:i+required_len]
                
                # è®¡ç®—RULï¼ˆå‰©ä½™ä½¿ç”¨å¯¿å‘½ï¼‰
                # ç®€åŒ–ï¼šå¦‚æœå½“å‰æ— æ•…éšœï¼ŒRULä¸ºåˆ°æ•…éšœå‘ç”Ÿçš„æ—¶é—´ï¼›å¦‚æœæœ‰æ•…éšœï¼ŒRULä¸º0
                current_fault = target_array[i+self.seq_len, 0]  # é¢„æµ‹èµ·ç‚¹çš„æ•…éšœçŠ¶æ€
                if current_fault == 0:
                    future_faults = target_array[i+self.seq_len:, 0]
                    fault_indices = np.where(future_faults == 1)[0]
                    if len(fault_indices) > 0:
                        first_fault_idx = fault_indices[0]
                        rul = first_fault_idx * 0.001  # è½¬æ¢ä¸ºç§’
                    else:
                        rul = 3600  # é»˜è®¤1å°æ—¶
                else:
                    rul = 0
                
                # RULå½’ä¸€åŒ–ï¼ˆç®€åŒ–ï¼‰
                rul_normalized = min(rul, 3600) / 3600
                
                samples.append({
                    'x_hist': x_hist,
                    'x_future_ctrl': x_future_ctrl,
                    'y_future_state': y_future_state,
                    'y_fault_label': y_targets[0, 0],  # å½“å‰æ­¥çš„æ•…éšœæ ‡ç­¾
                    'y_fault_type': y_targets[0, 1],   # å½“å‰æ­¥çš„æ•…éšœç±»å‹
                    'y_rul': rul_normalized
                })
                
                count += 1
                if count % 10000 == 0:
                    print(f"  å·²æ”¶é›† {count} ä¸ªæ ·æœ¬...")
                
                if count >= self.max_samples:
                    break
            
            if count >= self.max_samples:
                break
        
        self.total_samples = len(samples)
        self.split_idx = int(self.total_samples * 0.8)
        train_samples = samples[:self.split_idx]
        val_samples = samples[self.split_idx:]
        
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {self.total_samples}")
        print(f"   è®­ç»ƒé›†: {len(train_samples)}, éªŒè¯é›†: {len(val_samples)}")
        
        # è®¡ç®—ç»Ÿè®¡é‡ï¼ˆä»…ä½¿ç”¨è®­ç»ƒé›†ï¼‰
        print("ğŸ“Š è®¡ç®—ç»Ÿè®¡é‡...")
        all_x_hist = np.array([s['x_hist'] for s in train_samples])
        
        self.mean_X = all_x_hist.mean(axis=(0, 1))
        self.std_X = all_x_hist.std(axis=(0, 1))
        self.std_X[self.std_X < 1e-8] = 1.0
        
        print(f"   Input Mean: {self.mean_X}")
        print(f"   Input Std: {self.std_X}")
        
        # æ›´æ–°é…ç½®ä¸­çš„input_dimä»¥åæ˜ å®é™…ç»´åº¦
        self.config.input_dim = all_x_hist.shape[2]  # åº”è¯¥æ˜¯19ï¼ˆ18ä¸ªç‰¹å¾+1å°æ—¶ç‰¹å¾ï¼‰
        print(f"   å®é™…è¾“å…¥ç»´åº¦: {self.config.input_dim}")
        
        # å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®
        self.prepare_datasets(train_samples, val_samples)
        
        # ä¿å­˜å½’ä¸€åŒ–å‚æ•°
        self.save_normalization_params()
        
        print(f"âœ… æ•°æ®å¤„ç†å®Œæˆï¼")
    
    def prepare_datasets(self, train_samples, val_samples):
        """å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†"""
        def prepare_batch(samples):
            X_hist = np.zeros((len(samples), self.seq_len, self.config.input_dim), dtype=np.float32)
            X_ctrl = np.zeros((len(samples), self.pred_len, self.config.ctrl_dim), dtype=np.float32)
            Y_state = np.zeros((len(samples), self.pred_len, len(self.config.state_cols)), dtype=np.float32)
            Y_fault = np.zeros(len(samples), dtype=np.int64)
            Y_fault_type = np.zeros(len(samples), dtype=np.int64)
            Y_rul = np.zeros(len(samples), dtype=np.float32)
            
            for idx, sample in enumerate(samples):
                # ç¡®ä¿sample['x_hist']çš„ç»´åº¦ä¸input_dimä¸€è‡´
                hist_data = sample['x_hist']
                if hist_data.shape[1] != self.config.input_dim:
                    print(f"è­¦å‘Š: x_histç»´åº¦ä¸åŒ¹é…ã€‚æœŸæœ›: {self.config.input_dim}ï¼Œå®é™…: {hist_data.shape[1]}")
                    # æ ¹æ®å®é™…ç»´åº¦è°ƒæ•´mean_Xå’Œstd_X
                    if hist_data.shape[1] > self.config.input_dim:
                        # å¦‚æœå®é™…ç»´åº¦æ›´å¤§ï¼Œæˆªå–åˆ°é…ç½®çš„ç»´åº¦
                        hist_data = hist_data[:, :self.config.input_dim]
                    elif hist_data.shape[1] < self.config.input_dim:
                        # å¦‚æœå®é™…ç»´åº¦æ›´å°ï¼Œéœ€è¦é‡æ–°è®¡ç®—ç»Ÿè®¡å€¼
                        print(f"é”™è¯¯: å®é™…ç»´åº¦å°äºé…ç½®ç»´åº¦ï¼Œæ— æ³•å¤„ç†")
                        raise ValueError(f"æ•°æ®ç»´åº¦ä¸åŒ¹é…: æœŸæœ›è‡³å°‘{self.config.input_dim}ï¼Œå®é™…{hist_data.shape[1]}")
                
                X_hist[idx] = (hist_data - self.mean_X) / self.std_X
                
                ctrl_data = sample['x_future_ctrl']
                if ctrl_data.shape[1] != self.config.ctrl_dim:
                    print(f"è­¦å‘Š: x_future_ctrlç»´åº¦ä¸åŒ¹é…ã€‚æœŸæœ›: {self.config.ctrl_dim}ï¼Œå®é™…: {ctrl_data.shape[1]}")
                    if ctrl_data.shape[1] > self.config.ctrl_dim:
                        ctrl_data = ctrl_data[:, :self.config.ctrl_dim]
                    else:
                        print(f"é”™è¯¯: å®é™…æ§åˆ¶ç»´åº¦å°äºé…ç½®ç»´åº¦ï¼Œæ— æ³•å¤„ç†")
                        raise ValueError(f"æ§åˆ¶æ•°æ®ç»´åº¦ä¸åŒ¹é…: æœŸæœ›è‡³å°‘{self.config.ctrl_dim}ï¼Œå®é™…{ctrl_data.shape[1]}")
                
                X_ctrl[idx] = (ctrl_data - self.mean_X[:self.config.ctrl_dim]) / self.std_X[:self.config.ctrl_dim]
                
                # Y_state ä¹Ÿéœ€è¦å½’ä¸€åŒ–ï¼Œä½¿ç”¨ç›¸åŒçš„æ–¹æ³•
                state_data = sample['y_future_state']
                if state_data.shape[1] != len(self.config.state_cols):
                    print(f"è­¦å‘Š: y_future_stateç»´åº¦ä¸åŒ¹é…ã€‚æœŸæœ›: {len(self.config.state_cols)}ï¼Œå®é™…: {state_data.shape[1]}")
                    if state_data.shape[1] > len(self.config.state_cols):
                        state_data = state_data[:, :len(self.config.state_cols)]
                    else:
                        print(f"é”™è¯¯: å®é™…çŠ¶æ€ç»´åº¦å°äºé…ç½®ç»´åº¦ï¼Œæ— æ³•å¤„ç†")
                        raise ValueError(f"çŠ¶æ€æ•°æ®ç»´åº¦ä¸åŒ¹é…: æœŸæœ›è‡³å°‘{len(self.config.state_cols)}ï¼Œå®é™…{state_data.shape[1]}")
                
                Y_state[idx] = (state_data - self.mean_X[len(self.config.ctrl_cols):len(self.config.ctrl_cols)+len(self.config.state_cols)]) / \
                              self.std_X[len(self.config.ctrl_cols):len(self.config.ctrl_cols)+len(self.config.state_cols)]
                Y_fault[idx] = int(sample['y_fault_label'])
                Y_fault_type[idx] = int(sample['y_fault_type'])
                Y_rul[idx] = sample['y_rul']
            
            return X_hist, X_ctrl, Y_state, Y_fault, Y_fault_type, Y_rul
        
        self.train_X, self.train_ctrl, self.train_Y_state, self.train_Y_fault, self.train_Y_fault_type, self.train_Y_rul = prepare_batch(train_samples)
        self.val_X, self.val_ctrl, self.val_Y_state, self.val_Y_fault, self.val_Y_fault_type, self.val_Y_rul = prepare_batch(val_samples)
    
    def save_normalization_params(self):
        """ä¿å­˜å½’ä¸€åŒ–å‚æ•°"""
        params = {
            'mean_X': self.mean_X,
            'std_X': self.std_X,
            'ctrl_cols': self.config.ctrl_cols,
            'state_cols': self.config.state_cols
        }
        
        params_path = os.path.join(self.config.checkpoint_dir, 'normalization_params.pkl')
        os.makedirs(self.config.checkpoint_dir, exist_ok=True)
        
        with open(params_path, 'wb') as f:
            pickle.dump(params, f)
        
        print(f"ğŸ’¾ å½’ä¸€åŒ–å‚æ•°å·²ä¿å­˜: {params_path}")

# ==================== æ•°æ®é›†ç±» ====================
class MultiTaskDataset(Dataset):
    def __init__(self, X_hist, X_ctrl, Y_state, Y_fault, Y_fault_type, Y_rul):
        self.X_hist = torch.from_numpy(X_hist)
        self.X_ctrl = torch.from_numpy(X_ctrl)
        self.Y_state = torch.from_numpy(Y_state)
        self.Y_fault = torch.from_numpy(Y_fault)
        self.Y_fault_type = torch.from_numpy(Y_fault_type)
        self.Y_rul = torch.from_numpy(Y_rul)
    
    def __len__(self):
        return self.X_hist.shape[0]
    
    def __getitem__(self, idx):
        return {
            'x_hist': self.X_hist[idx],
            'x_ctrl': self.X_ctrl[idx],
            'y_state': self.Y_state[idx],
            'y_fault': self.Y_fault[idx],
            'y_fault_type': self.Y_fault_type[idx],
            'y_rul': self.Y_rul[idx]
        }

def multitask_collate_fn(batch):
    """è‡ªå®šä¹‰çš„collateå‡½æ•°"""
    x_hist = torch.stack([item['x_hist'] for item in batch])
    x_ctrl = torch.stack([item['x_ctrl'] for item in batch])
    y_state = torch.stack([item['y_state'] for item in batch])
    y_fault = torch.stack([item['y_fault'] for item in batch])
    y_fault_type = torch.stack([item['y_fault_type'] for item in batch])
    y_rul = torch.stack([item['y_rul'] for item in batch])
    
    return {
        'x_hist': x_hist,
        'x_ctrl': x_ctrl,
        'y_state': y_state,
        'y_fault': y_fault,
        'y_fault_type': y_fault_type,
        'y_rul': y_rul
    }

# ==================== è®­ç»ƒå‡½æ•° ====================
def train_multitask_pinn(config):
    print("=" * 80)
    print("ğŸš€ PrinterPINN å¤šä»»åŠ¡è®­ç»ƒ (ç‰©ç†åœºé‡æ„ + æ•…éšœåˆ†ç±» + RULé¢„æµ‹)")
    print("=" * 80)
    
    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    os.makedirs(config.checkpoint_dir, exist_ok=True)
    
    # æ•°æ®å¤„ç†
    processor = MultiTaskDataProcessor(
        config.data_path,
        config.seq_len,
        config.pred_len,
        config.max_samples,
        config
    )
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = MultiTaskDataset(
        processor.train_X, processor.train_ctrl, processor.train_Y_state,
        processor.train_Y_fault, processor.train_Y_fault_type, processor.train_Y_rul
    )
    
    val_dataset = MultiTaskDataset(
        processor.val_X, processor.val_ctrl, processor.val_Y_state,
        processor.val_Y_fault, processor.val_Y_fault_type, processor.val_Y_rul
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,  # Windowsè®¾ä¸º0æˆ–2
        pin_memory=config.pin_memory,    # å¿…é¡»ä¸ºTrue
        persistent_workers=True,         # å‡å°‘workeré‡å»ºå¼€é”€
        prefetch_factor=2 if config.num_workers > 0 else None
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=True,
        collate_fn=multitask_collate_fn
    )
    
    # æ¨¡å‹
    model = PrinterPINN_MultiTask(config)
    if torch.cuda.device_count() > 1:
        print(f"ğŸ® ä½¿ç”¨ {torch.cuda.device_count()} ä¸ª GPU!")
        model = nn.DataParallel(model)
    model = model.to(config.device)
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.lr,
        betas=(0.9, 0.999),
        weight_decay=1e-5
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    warmup_scheduler = LinearLR(
        optimizer,
        start_factor=0.1,
        end_factor=1.0,
        total_iters=config.warmup_epochs * len(train_loader)
    )
    
    cosine_scheduler = CosineAnnealingLR(
        optimizer,
        T_max=(config.epochs - config.warmup_epochs) * len(train_loader),
        eta_min=1e-6
    )
    
    scheduler = SequentialLR(
        optimizer,
        schedulers=[warmup_scheduler, cosine_scheduler],
        milestones=[config.warmup_epochs * len(train_loader)]
    )
    
    # æŸå¤±å‡½æ•°
    physics_criterion = nn.MSELoss()
    class_criterion = nn.CrossEntropyLoss()
    rul_criterion = nn.MSELoss()
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler('cuda', enabled=True)
    
    # TensorBoard
    log_dir = os.path.join("runs", f"multitask_{time.strftime('%Y%m%d_%H%M%S')}")
    os.makedirs(log_dir, exist_ok=True)
    writer = SummaryWriter(log_dir)
    
    # ä»æ£€æŸ¥ç‚¹æ¢å¤
    start_epoch = 0
    best_val_loss = float('inf')
    
    if config.resume_from is not None and os.path.exists(config.resume_from):
        checkpoint = torch.load(config.resume_from, map_location='cpu')
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch']
        best_val_loss = checkpoint['best_loss']
        if 'scaler_state_dict' in checkpoint and scaler is not None:
            scaler.load_state_dict(checkpoint['scaler_state_dict'])
        print(f"âœ… æ£€æŸ¥ç‚¹å·²åŠ è½½: {config.resume_from}")
        print(f"   ä»Epoch {start_epoch}å¼€å§‹ç»§ç»­è®­ç»ƒ")
        print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")

    # è®­ç»ƒå¾ªç¯
    print_every = 50
    print("\nğŸš€ å¼€å§‹å¤šä»»åŠ¡è®­ç»ƒ...")
    print(f"{'='*80}")
    
    # è®°å½•æ¯ä¸ªepochçš„æ—¶é—´ï¼Œç”¨äºé¢„æµ‹å‰©ä½™æ—¶é—´
    epoch_times = []
    
    for epoch in range(start_epoch, config.epochs):
        epoch_start = time.time()
        model.train()
        
        total_physics_loss = 0
        total_class_loss = 0
        total_rul_loss = 0
        total_physics_loss_term = 0
        
        optimizer.zero_grad()
        
        for batch_idx, batch in enumerate(train_loader):
            # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            x_hist = batch['x_hist'].to(config.device)
            x_ctrl = batch['x_ctrl'].to(config.device)
            y_state = batch['y_state'].to(config.device)
            y_fault = batch['y_fault'].to(config.device)
            y_fault_type = batch['y_fault_type'].to(config.device)
            y_rul = batch['y_rul'].to(config.device)
            
            # å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
            with autocast(device_type='cuda', enabled=True):
                # å‰å‘ä¼ æ’­
                outputs = model(x_hist, x_ctrl)
                
                # 1. ç‰©ç†åœºé‡æ„æŸå¤±
                physics_loss = physics_criterion(outputs['physics_pred'], y_state)
                
                # 2. æ•…éšœåˆ†ç±»æŸå¤±
                # å°†æ•…éšœç±»å‹è½¬æ¢ä¸ºåˆ†ç±»æ ‡ç­¾ï¼ˆ0=æ­£å¸¸ï¼Œ1-3=æ•…éšœç±»å‹ï¼‰
                class_labels = torch.zeros_like(y_fault, dtype=torch.long)
                mask_fault = (y_fault == 1)
                class_labels[mask_fault] = y_fault_type[mask_fault].long()
                # ç¡®ä¿æ ‡ç­¾åœ¨[0, class_dim-1]èŒƒå›´å†…
                class_labels = torch.clamp(class_labels, 0, config.class_dim-1)
                
                class_loss = class_criterion(outputs['class_pred'], class_labels)
                
                # 3. RULå›å½’æŸå¤±
                rul_loss = rul_criterion(outputs['rul_pred'].squeeze(), y_rul)
                
                # 4. ç‰©ç†çº¦æŸæŸå¤±
                # è§£å†³DataParallelæ— æ³•è®¿é—®è‡ªå®šä¹‰æ–¹æ³•çš„é—®é¢˜
                if isinstance(model, nn.DataParallel):
                    physics_constraint_loss = model.module.physics_loss(outputs, y_state, config.device)
                else:
                    physics_constraint_loss = model.physics_loss(outputs, y_state, config.device)
                
                # é˜²æ­¢ç‰©ç†çº¦æŸæŸå¤±ä¸ºæ— ç©·å¤§æˆ–NaN
                if torch.isnan(physics_constraint_loss) or torch.isinf(physics_constraint_loss):
                    print(f"âš ï¸  æ£€æµ‹åˆ°ç‰©ç†çº¦æŸæŸå¤±å¼‚å¸¸: {physics_constraint_loss}")
                    physics_constraint_loss = torch.tensor(0.0, device=physics_constraint_loss.device, dtype=physics_constraint_loss.dtype)
                
                # æ€»æŸå¤±
                total_loss = (physics_loss + 
                             config.lambda_classification * class_loss + 
                             config.lambda_rul * rul_loss + 
                             config.lambda_physics * physics_constraint_loss)
                
                # æ£€æŸ¥æ€»æŸå¤±æ˜¯å¦æ­£å¸¸
                if torch.isnan(total_loss) or torch.isinf(total_loss):
                    print(f"âš ï¸  æ£€æµ‹åˆ°æ€»æŸå¤±å¼‚å¸¸: {total_loss}")
                    continue  # è·³è¿‡è¿™ä¸ªæ‰¹æ¬¡
            
            # åå‘ä¼ æ’­ - ä½¿ç”¨scalerè¿›è¡Œç¼©æ”¾
            scaler.scale(total_loss).backward()
            
            if (batch_idx + 1) % config.gradient_accumulation_steps == 0:
                # Unscales gradients for the optimizer step
                scaler.unscale_(optimizer)
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                # æ›´æ–°ä¼˜åŒ–å™¨å‚æ•°
                scaler.step(optimizer)
                # æ›´æ–°scalerçŠ¶æ€
                scaler.update()
                # æ¸…é›¶æ¢¯åº¦
                optimizer.zero_grad()
                # æ›´æ–°å­¦ä¹ ç‡
                scheduler.step()
            
            # ç´¯ç§¯æŸå¤±
            total_physics_loss += physics_loss.item()
            total_class_loss += class_loss.item()
            total_rul_loss += rul_loss.item()
            total_physics_loss_term += physics_constraint_loss.item()
            
            # æ‰“å°è¿›åº¦
            if (batch_idx + 1) % print_every == 0:
                avg_physics = total_physics_loss / (batch_idx + 1)
                avg_class = total_class_loss / (batch_idx + 1)
                avg_rul = total_rul_loss / (batch_idx + 1)
                avg_physics_term = total_physics_loss_term / (batch_idx + 1)
                
                current_lr = optimizer.param_groups[0]['lr']
                print(f"  ğŸ”µ Epoch {epoch+1:2d}/{config.epochs} | Batch {batch_idx+1:4d}/{len(train_loader):4d} | "
                      f"Physics: {avg_physics:.4f} | Class: {avg_class:.4f} | RUL: {avg_rul:.4f} | "
                      f"PhysicsTerm: {avg_physics_term:.4f} | LR: {current_lr:.2e}")
        
        # éªŒè¯
        model.eval()
        val_physics_loss = 0
        val_class_loss = 0
        val_rul_loss = 0
        val_physics_term = 0
        
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in val_loader:
                x_hist = batch['x_hist'].to(config.device)
                x_ctrl = batch['x_ctrl'].to(config.device)
                y_state = batch['y_state'].to(config.device)
                y_fault = batch['y_fault'].to(config.device)
                y_fault_type = batch['y_fault_type'].to(config.device)
                y_rul = batch['y_rul'].to(config.device)
                
                # åœ¨éªŒè¯æœŸé—´ä¹Ÿä½¿ç”¨æ··åˆç²¾åº¦
                with autocast(device_type='cuda', enabled=True):
                    outputs = model(x_hist, x_ctrl)
                    
                    # ç‰©ç†åœºé‡æ„æŸå¤±
                    physics_loss = physics_criterion(outputs['physics_pred'], y_state)
                    
                    # æ•…éšœåˆ†ç±»
                    class_labels = torch.zeros_like(y_fault, dtype=torch.long)
                    mask_fault = (y_fault == 1)
                    class_labels[mask_fault] = y_fault_type[mask_fault].long()
                    class_labels = torch.clamp(class_labels, 0, config.class_dim-1)
                    
                    class_loss = class_criterion(outputs['class_pred'], class_labels)
                    
                    # RULæŸå¤±
                    rul_loss = rul_criterion(outputs['rul_pred'].squeeze(), y_rul)
                    
                    # ç‰©ç†çº¦æŸ
                    # è§£å†³DataParallelæ— æ³•è®¿é—®è‡ªå®šä¹‰æ–¹æ³•çš„é—®é¢˜
                    if isinstance(model, nn.DataParallel):
                        physics_term = model.module.physics_loss(outputs, y_state, config.device)
                    else:
                        physics_term = model.physics_loss(outputs, y_state, config.device)
                    
                    # æ£€æŸ¥ç‰©ç†é¡¹æ˜¯å¦ä¸ºå¼‚å¸¸å€¼
                    if torch.isnan(physics_term) or torch.isinf(physics_term):
                        print(f"âš ï¸  éªŒè¯æœŸé—´æ£€æµ‹åˆ°ç‰©ç†çº¦æŸæŸå¤±å¼‚å¸¸: {physics_term}")
                        physics_term = torch.tensor(0.0, device=physics_term.device, dtype=physics_term.dtype)
                
                val_physics_loss += physics_loss.item()
                val_class_loss += class_loss.item()
                val_rul_loss += rul_loss.item()
                val_physics_term += physics_term.item()
                
                # æ”¶é›†é¢„æµ‹ç»“æœç”¨äºè¯„ä¼°
                _, predicted = torch.max(outputs['class_pred'], 1)
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(class_labels.cpu().numpy())
        
        # è®¡ç®—å½“å‰epochè€—æ—¶
        epoch_time = time.time() - epoch_start
        epoch_times.append(epoch_time)
        
        # è®¡ç®—å¹³å‡epochæ—¶é—´å¹¶é¢„æµ‹å‰©ä½™æ—¶é—´
        avg_epoch_time = sum(epoch_times) / len(epoch_times)
        remaining_epochs = config.epochs - (epoch + 1)
        remaining_time = avg_epoch_time * remaining_epochs
        
        # å°†å‰©ä½™æ—¶é—´è½¬æ¢ä¸ºå°æ—¶ã€åˆ†é’Ÿã€ç§’
        hours = int(remaining_time // 3600)
        minutes = int((remaining_time % 3600) // 60)
        seconds = int(remaining_time % 60)
        
        # è®¡ç®—å¹³å‡éªŒè¯æŸå¤±
        avg_val_physics = val_physics_loss / len(val_loader)
        avg_val_class = val_class_loss / len(val_loader)
        avg_val_rul = val_rul_loss / len(val_loader)
        avg_val_physics_term = val_physics_term / len(val_loader)
        
        total_val_loss = avg_val_physics + config.lambda_classification * avg_val_class + \
                        config.lambda_rul * avg_val_rul + config.lambda_physics * avg_val_physics_term

        # æ‰“å°epochæ‘˜è¦
        print(f"ğŸŸ¢ Epoch {epoch+1:2d}/{config.epochs} | Time: {epoch_time:.2f}s | ETA: {hours:02d}h {minutes:02d}m {seconds:02d}s")
        print(f"   Train - Physics: {total_physics_loss/len(train_loader):.4f} | "
              f"Class: {total_class_loss/len(train_loader):.4f} | "
              f"RUL: {total_rul_loss/len(train_loader):.4f} | "
              f"PhysicsTerm: {total_physics_loss_term/len(train_loader):.4f}")
        print(f"   Val   - Physics: {avg_val_physics:.4f} | "
              f"Class: {avg_val_class:.4f} | "
              f"RUL: {avg_val_rul:.4f} | "
              f"PhysicsTerm: {avg_val_physics_term:.4f} | "
              f"Total: {total_val_loss:.4f}")
        
        # åˆ†ç±»æ€§èƒ½è¯„ä¼°
        if len(all_preds) > 0 and len(set(all_labels)) > 1:  # ç¡®ä¿è‡³å°‘æœ‰ä¸¤ä¸ªä¸åŒçš„æ ‡ç­¾
            print("\nğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
            # æ£€æŸ¥å®é™…çš„æ ‡ç­¾æ•°é‡ï¼Œåªæ˜¾ç¤ºå®é™…å­˜åœ¨çš„ç±»åˆ«
            unique_labels = sorted(set(all_labels))
            if len(unique_labels) > 1:  # ç¡®ä¿æœ‰å¤šä¸ªç±»åˆ«
                target_names_map = {
                    0: 'Normal', 
                    1: 'Nozzle Clog', 
                    2: 'Mechanical Loose', 
                    3: 'Motor Fault'
                }
                actual_target_names = [target_names_map[i] for i in unique_labels if i in target_names_map]
                
                print(classification_report(
                    all_labels, 
                    all_preds, 
                    labels=unique_labels,
                    target_names=actual_target_names
                ))
            else:
                print(f"âš ï¸  åªæœ‰ä¸€ä¸ªç±»åˆ«è¢«é¢„æµ‹ï¼Œæ— æ³•ç”Ÿæˆåˆ†ç±»æŠ¥å‘Šã€‚å”¯ä¸€æ ‡ç­¾: {unique_labels[0]}")
            
            # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
            plt.figure(figsize=(8, 6))
            cm = confusion_matrix(all_labels, all_preds)
            # ç¡®ä¿æ ‡ç­¾é¡ºåºæ­£ç¡®
            unique_all = sorted(set(all_labels + all_preds))
            target_names_map = {
                0: 'Normal', 
                1: 'Nozzle Clog', 
                2: 'Mechanical Loose', 
                3: 'Motor Fault'
            }
            tick_labels = [target_names_map.get(i, f'Class {i}') for i in unique_all]
            
            sns.heatmap(
                cm, 
                annot=True, 
                fmt='d', 
                cmap='Blues',
                xticklabels=tick_labels,
                yticklabels=tick_labels
            )
            plt.title(f'Confusion Matrix - Epoch {epoch+1}')
            plt.xlabel('Predicted')
            plt.ylabel('True')
            
            cm_path = os.path.join(config.checkpoint_dir, f'confusion_matrix_epoch{epoch+1}.png')
            plt.savefig(cm_path)
            plt.close()
            
            print(f"   æ··æ·†çŸ©é˜µå·²ä¿å­˜: {cm_path}")
        else:
            print(f"âš ï¸  Epoch {epoch+1}: æ— æ³•ç”Ÿæˆåˆ†ç±»æŠ¥å‘Šï¼Œé¢„æµ‹æ•°æ®ä¸è¶³æˆ–ç±»åˆ«ä¸å…¨")
        
        # TensorBoardè®°å½•
        writer.add_scalar("Loss/train_physics", total_physics_loss/len(train_loader), epoch)
        writer.add_scalar("Loss/train_class", total_class_loss/len(train_loader), epoch)
        writer.add_scalar("Loss/train_rul", total_rul_loss/len(train_loader), epoch)
        writer.add_scalar("Loss/train_physics_term", total_physics_loss_term/len(train_loader), epoch)
        
        writer.add_scalar("Loss/val_physics", avg_val_physics, epoch)
        writer.add_scalar("Loss/val_class", avg_val_class, epoch)
        writer.add_scalar("Loss/val_rul", avg_val_rul, epoch)
        writer.add_scalar("Loss/val_physics_term", avg_val_physics_term, epoch)
        writer.add_scalar("Loss/val_total", total_val_loss, epoch)
        
        writer.add_scalar("Time/epoch", epoch_time, epoch)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if total_val_loss < best_val_loss:
            best_val_loss = total_val_loss
            checkpoint_path = os.path.join(config.checkpoint_dir, "best_multitask_model.pth")
            save_checkpoint(epoch+1, model, optimizer, scheduler, total_val_loss, best_val_loss, config, checkpoint_path, scaler)
            print(f"  ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (éªŒè¯æŸå¤±: {best_val_loss:.4f})")
        
        # å®šæœŸä¿å­˜
        if (epoch + 1) % config.save_interval == 0:
            checkpoint_path = os.path.join(config.checkpoint_dir, f"checkpoint_epoch{epoch+1}.pth")
            save_checkpoint(epoch+1, model, optimizer, scheduler, total_val_loss, best_val_loss, config, checkpoint_path, scaler)
    
    print(f"\n{'='*80}")
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"{'='*80}")

def save_checkpoint(epoch, model, optimizer, scheduler, current_loss, best_loss, config, filename, scaler=None):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'current_loss': current_loss,
        'best_loss': best_loss,
        'config': config.__dict__,
    }
    if scaler is not None:
        checkpoint['scaler_state_dict'] = scaler.state_dict()
    torch.save(checkpoint, filename)
    print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}")

def load_checkpoint(model, optimizer, scheduler, filename):
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    checkpoint = torch.load(filename, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    start_epoch = checkpoint['epoch']
    best_loss = checkpoint['best_loss']
    
    print(f"âœ… æ£€æŸ¥ç‚¹å·²åŠ è½½: {filename}")
    print(f"   ä»Epoch {start_epoch}å¼€å§‹ç»§ç»­è®­ç»ƒ")
    print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_loss:.6f}")
    
    return start_epoch, best_loss

# ==================== ä¸»å‡½æ•° ====================
def get_args():
    parser = argparse.ArgumentParser(description='è®­ç»ƒ3Dæ‰“å°æœºå¤šä»»åŠ¡PINNæ¨¡å‹')
    parser.add_argument('--data_path', type=str, default='printer_dataset/nozzle_simulation_gear_print.csv',
                        help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=2048, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--resume_from', type=str, help='ä»æŒ‡å®šæ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    parser.add_argument('--checkpoint_dir', type=str, default='./checkpoints_multitask', help='æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•')
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu', help='è®¾å¤‡')
    return parser.parse_args()

if __name__ == "__main__":
    args = get_args()
    config = Config()
    
    # æ›´æ–°é…ç½®
    config.data_path = args.data_path
    config.epochs = args.epochs
    config.batch_size = args.batch_size
    config.lr = args.lr
    config.resume_from = args.resume_from
    config.checkpoint_dir = args.checkpoint_dir
    config.device = args.device
    
    train_multitask_pinn(config)