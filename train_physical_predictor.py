# train_physical_predictor_enhanced.py
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
import os
import time
import pickle
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR
from torch.amp import autocast, GradScaler
import argparse
from datetime import datetime, timedelta
import seaborn as sns
from scipy import signal
import warnings
warnings.filterwarnings('ignore')

# ==================== é…ç½®å‚æ•° ====================
class Config:
    def __init__(self, resume_from=None, gpu_ids=[0]):
        self.data_path = 'printer_dataset_correction/printer_gear_correction_dataset.csv'
        self.seq_len = 250           # å†å²çª—å£é•¿åº¦ (250ms)
        self.pred_len = 50           # é¢„æµ‹é•¿åº¦ (50ms)
        self.batch_size = 1024
        self.gradient_accumulation_steps = 2
        self.model_dim = 192
        self.num_heads = 8
        self.num_layers = 5
        self.dim_feedforward = 768
        self.dropout = 0.1
        self.lr = 5e-5
        self.epochs = 60
        self.gpu_ids = gpu_ids
        self.resume_from = resume_from
        
        if len(gpu_ids) > 1:
            self.device = f'cuda:{gpu_ids[0]}'  # ä¸»GPU
        else:
            self.device = f'cuda:{gpu_ids[0]}' if torch.cuda.is_available() else 'cpu'
            
        self.lambda_physics = 0.4    # ç‰©ç†çº¦æŸæƒé‡
        self.lambda_freq = 0.3       # é¢‘åŸŸçº¦æŸæƒé‡
        self.checkpoint_dir = './checkpoints_physical_predictor_enhanced'
        self.max_samples = 300000
        self.warmup_epochs = 5
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # ç‰¹å¾åˆ—
        self.feature_cols = [
            'ctrl_T_target', 'ctrl_speed_set', 'ctrl_pos_x', 'ctrl_pos_y', 'ctrl_pos_z',
            'temperature_C', 'vibration_disp_x_m', 'vibration_disp_y_m',
            'vibration_vel_x_m_s', 'vibration_vel_y_m_s',
            'motor_current_x_A', 'motor_current_y_A',
            'pressure_bar'
        ]
        
        # ç›®æ ‡åˆ— (éœ€è¦é¢„æµ‹çš„ç‰©ç†é‡)
        self.target_cols = [
            'vibration_disp_x_m', 'vibration_disp_y_m',
            'temperature_C', 'motor_current_x_A', 'motor_current_y_A'
        ]
        
        # é¢‘åŸŸç‰¹å¾å‚æ•°
        self.freq_bands = 8  # é¢‘ç‡å¸¦æ•°é‡
        self.sampling_rate = 1000  # 1kHz é‡‡æ ·ç‡ (1msæ­¥é•¿)
        self.max_freq = 500  # æœ€å¤§é¢‘ç‡ (Hz)
        
        # è®¡ç®—æ€»è¾“å…¥ç»´åº¦ (æ—¶åŸŸ + é¢‘åŸŸ)
        self.time_domain_dim = len(self.feature_cols)
        self.freq_domain_dim = self.freq_bands * len(self.feature_cols)
        self.input_dim = self.time_domain_dim + self.freq_domain_dim
        self.output_dim = len(self.target_cols)

# ==================== ä½ç½®ç¼–ç  ====================
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:, :x.size(1), :]

# è‡ªå®šä¹‰æ»¤æ³¢å™¨æ¨¡å—
class FilterModule(nn.Module):
    def __init__(self, b, a):
        super().__init__()
        self.register_buffer('b', torch.tensor(b, dtype=torch.float32))
        self.register_buffer('a', torch.tensor(a, dtype=torch.float32))
    
    def forward(self, x):
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„é¢‘åŸŸåˆ†æï¼Œå› ä¸ºå®é™…çš„æ»¤æ³¢å®ç°æ¯”è¾ƒå¤æ‚
        # æˆ‘ä»¬ç›´æ¥è¿”å›è¾“å…¥ï¼Œå› ä¸ºå®é™…çš„é¢‘åŸŸç‰¹å¾å·²ç»åœ¨é¢„å¤„ç†ä¸­è®¡ç®—äº†
        return x

# ==================== é¢‘åŸŸç‰¹å¾æå–å™¨ ====================
class FrequencyFeatureExtractor(nn.Module):
    def __init__(self, config):
        super(FrequencyFeatureExtractor, self).__init__()
        self.config = config
        
        # å®šä¹‰é¢‘ç‡å¸¦ (å¯¹æ•°å°ºåº¦)
        self.freq_bands = np.logspace(np.log10(1), np.log10(config.max_freq), config.freq_bands + 1)

    def forward(self, x):
        """
        x: [batch, seq_len, input_dim]
        ç”±äºé¢‘åŸŸç‰¹å¾å·²åœ¨é¢„å¤„ç†é˜¶æ®µè®¡ç®—å¹¶æ·»åŠ åˆ°è¾“å…¥ä¸­ï¼Œè¿™é‡Œæˆ‘ä»¬æå–è¿™äº›ç‰¹å¾
        """
        batch_size, seq_len, input_dim = x.shape
        device = x.device
        
        # è®¡ç®—åŸå§‹æ—¶åŸŸç‰¹å¾æ•°é‡
        original_features = len(self.config.feature_cols)
        freq_features_count = input_dim - original_features
        
        if freq_features_count <= 0:
            # å¦‚æœæ²¡æœ‰é¢‘åŸŸç‰¹å¾ï¼Œè¿”å›ç©ºå¼ é‡
            return torch.zeros(batch_size, seq_len, 0, device=device)
        
        # æå–é¢‘åŸŸç‰¹å¾éƒ¨åˆ†ï¼ˆå³é™¤äº†åŸå§‹ç‰¹å¾ä¹‹å¤–çš„éƒ¨åˆ†ï¼‰
        freq_part = x[:, :, len(self.config.feature_cols):]  # æå–é¢‘åŸŸéƒ¨åˆ†
        
        return freq_part

# ==================== å¢å¼ºç‰ˆç‰©ç†é¢„æµ‹æ¨¡å‹ ====================
class EnhancedPhysicalPredictor(nn.Module):
    def __init__(self, config):
        super(EnhancedPhysicalPredictor, self).__init__()
        self.config = config
        
        # é¢‘åŸŸç‰¹å¾æå– - ç°åœ¨åªæ˜¯æå–é¢„è®¡ç®—çš„é¢‘åŸŸç‰¹å¾
        self.freq_extractor = FrequencyFeatureExtractor(config)
        
        # ä½¿ç”¨é…ç½®ä¸­çš„æ€»è¾“å…¥ç»´åº¦ï¼ˆå·²åŒ…å«é¢‘åŸŸç‰¹å¾ï¼‰
        total_input_dim = config.input_dim
        
        # ç¼–ç å™¨ - ä½¿ç”¨æ›´æ–°åçš„è¾“å…¥ç»´åº¦
        self.encoder_embedding = nn.Linear(total_input_dim, config.model_dim)
        self.pos_encoder = PositionalEncoding(config.model_dim)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=config.model_dim,
            nhead=config.num_heads,
            dim_feedforward=config.dim_feedforward,
            dropout=config.dropout,
            batch_first=True,
            norm_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)
        
        # å¤šå¤´è¾“å‡º (é’ˆå¯¹ä¸åŒç‰©ç†é‡)
        self.vibration_head = nn.Sequential(
            nn.Linear(config.model_dim, 96),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(96, 2)  # x, yæŒ¯åŠ¨
        )
        
        self.thermal_head = nn.Sequential(
            nn.Linear(config.model_dim, 64),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(64, 1)  # æ¸©åº¦
        )
        
        self.motor_head = nn.Sequential(
            nn.Linear(config.model_dim, 64),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(64, 2)  # x, yç”µæœºç”µæµ
        )
        
        # ç‰©ç†å‚æ•° (å¯å­¦ä¹ )
        self.register_buffer('mass', torch.tensor(0.045))  # å–·å¤´è´¨é‡ (kg)
        self.register_buffer('stiffness', torch.tensor(1500.0))  # åˆšåº¦ (N/m)
        self.register_buffer('damping', torch.tensor(0.48))  # é˜»å°¼ç³»æ•°

    def forward(self, x):
        """
        x: [batch, seq_len, input_dim] - å·²åŒ…å«é¢„è®¡ç®—çš„é¢‘åŸŸç‰¹å¾
        """
        # xå·²ç»åŒ…å«äº†æ—¶åŸŸå’Œé¢‘åŸŸç‰¹å¾ï¼Œç›´æ¥ä½¿ç”¨å³å¯
        
        # ç¼–ç å™¨
        x_emb = self.encoder_embedding(x)
        x_emb = self.pos_encoder(x_emb)
        memory = self.encoder(x_emb)  # [batch, seq_len, model_dim]
        
        # ä½¿ç”¨åºåˆ—çš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥è¿›è¡Œé¢„æµ‹
        last_state = memory[:, -1, :]  # [batch, model_dim]
        
        # å¤šå¤´é¢„æµ‹
        vib_pred = self.vibration_head(last_state)  # [2]
        temp_pred = self.thermal_head(last_state)   # [1]
        motor_pred = self.motor_head(last_state)    # [2]
        
        # åˆå¹¶é¢„æµ‹ç»“æœ
        prediction = torch.cat([vib_pred, temp_pred, motor_pred], dim=1)  # [batch, 5]
        
        return prediction

    def physics_loss(self, predictions, targets, dt=0.001):
        """å¢å¼ºçš„ç‰©ç†çº¦æŸæŸå¤±"""
        loss = 0.0
        
        # 1. æŒ¯åŠ¨åŠ¨åŠ›å­¦çº¦æŸ (è´¨é‡-å¼¹ç°§-é˜»å°¼ç³»ç»Ÿ)
        vib_x_pred = predictions[:, 0]
        vib_y_pred = predictions[:, 1]
        
        # åŸå§‹ç›®æ ‡å€¼
        vib_x_target = targets[:, 0]
        vib_y_target = targets[:, 1]
        
        # æŒ¯åŠ¨åº”è¯¥å¹³æ»‘å˜åŒ–
        if len(vib_x_pred) > 1:
            vib_x_smoothness = torch.mean(torch.abs(torch.diff(vib_x_pred)))
            vib_y_smoothness = torch.mean(torch.abs(torch.diff(vib_y_pred)))
            loss += 0.3 * (vib_x_smoothness + vib_y_smoothness)
        
        # 2. çƒ­ä¼ å¯¼æ–¹ç¨‹çº¦æŸ
        temp_pred = predictions[:, 2]
        temp_target = targets[:, 2]
        
        if len(temp_pred) > 1:
            dT_dt = torch.diff(temp_pred) / dt
            # æ¸©åº¦å˜åŒ–ç‡åº”è¯¥å¹³æ»‘
            d2T_dt2 = torch.diff(dT_dt) / dt
            thermal_smoothness = torch.mean(torch.abs(d2T_dt2))
            loss += 0.2 * thermal_smoothness
            
            # æ¸©åº¦ä¸åº”è¯¥çªå˜
            temp_change = torch.mean(torch.abs(torch.diff(temp_pred)))
            loss += 0.3 * torch.clamp(temp_change - 1.0, min=0)  # æ¯æ¯«ç§’å˜åŒ–ä¸è¶…è¿‡1Â°C
        
        # 3. ç”µæœºç”µæµ-æŒ¯åŠ¨è€¦åˆçº¦æŸ
        current_x_pred = predictions[:, 3]
        current_y_pred = predictions[:, 4]
        
        # ç”µæœºç”µæµåº”è¯¥ä¸æŒ¯åŠ¨å¹…åº¦ç›¸å…³
        vib_magnitude = torch.sqrt(vib_x_pred**2 + vib_y_pred**2)
        current_magnitude = torch.sqrt(current_x_pred**2 + current_y_pred**2)
        
        # è®¡ç®—ç›¸å…³æ€§
        if len(vib_magnitude) > 1:
            vib_mean = torch.mean(vib_magnitude)
            current_mean = torch.mean(current_magnitude)
            
            vib_centered = vib_magnitude - vib_mean
            current_centered = current_magnitude - current_mean
            
            correlation = torch.sum(vib_centered * current_centered) / (
                torch.sqrt(torch.sum(vib_centered**2)) * 
                torch.sqrt(torch.sum(current_centered**2)) + 1e-8
            )
            # ç¡®ä¿ç›¸å…³æ€§å€¼åœ¨åˆç†èŒƒå›´å†…
            correlation = torch.clamp(correlation, -1.0, 1.0)
            
            # å¸Œæœ›æœ‰æ­£ç›¸å…³
            loss += 0.2 * torch.relu(0.3 - correlation)
        
        return loss

    def frequency_loss(self, predictions, targets):
        """é¢‘åŸŸä¸€è‡´æ€§æŸå¤±"""
        loss = 0.0
        
        # ç¡®ä¿å¼ é‡æ˜¯float32ç²¾åº¦ä»¥é¿å…FFTçš„åŠç²¾åº¦é—®é¢˜
        predictions = predictions.float()
        targets = targets.float()
        
        # è®¡ç®—é¢„æµ‹å€¼å’Œç›®æ ‡å€¼çš„FFT
        pred_x_fft = torch.fft.rfft(predictions[:, 0])  # é¢„æµ‹çš„xæŒ¯åŠ¨
        pred_y_fft = torch.fft.rfft(predictions[:, 1])  # é¢„æµ‹çš„yæŒ¯åŠ¨
        target_x_fft = torch.fft.rfft(targets[:, 0])    # ç›®æ ‡çš„xæŒ¯åŠ¨
        target_y_fft = torch.fft.rfft(targets[:, 1])    # ç›®æ ‡çš„yæŒ¯åŠ¨
        
        # å–å‰Nä¸ªé¢‘ç‡åˆ†é‡è¿›è¡Œæ¯”è¾ƒï¼Œç¡®ä¿ä¸ä¼šè¶…å‡ºç´¢å¼•èŒƒå›´
        max_freq_bins = min(10, pred_x_fft.shape[0])
        pred_x_mag = torch.abs(pred_x_fft[:max_freq_bins])
        pred_y_mag = torch.abs(pred_y_fft[:max_freq_bins])
        target_x_mag = torch.abs(target_x_fft[:max_freq_bins])
        target_y_mag = torch.abs(target_y_fft[:max_freq_bins])
        
        # é¢‘åŸŸå¹…åº¦æŸå¤±
        freq_mag_loss = nn.MSELoss()(pred_x_mag, target_x_mag) + \
                        nn.MSELoss()(pred_y_mag, target_y_mag)
        
        loss += freq_mag_loss
        
        return loss

# ==================== æ•°æ®é›†ç±» ====================
class PhysicalPredictionDataset(Dataset):
    def __init__(self, sequences, targets):
        self.sequences = torch.tensor(sequences, dtype=torch.float32)
        self.targets = torch.tensor(targets, dtype=torch.float32)
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        return self.sequences[idx], self.targets[idx]

# ==================== é¢‘åŸŸç‰¹å¾å¤„ç†å™¨ ====================
class FrequencyFeatureProcessor:
    """å†…å­˜å‹å¥½çš„é¢‘åŸŸç‰¹å¾å¤„ç†å™¨"""
    def __init__(self, config, feature_mean, feature_std):
        self.config = config
        self.feature_mean = feature_mean
        self.feature_std = feature_std
        
        # å®šä¹‰é¢‘ç‡å¸¦ (å¯¹æ•°å°ºåº¦)
        self.freq_bands = np.logspace(np.log10(1), np.log10(config.max_freq), 
                                     config.freq_bands + 1)
        
        # é¢„è®¡ç®—FFTé¢‘ç‡
        self.fft_freqs = np.fft.rfftfreq(config.seq_len, 1.0/config.sampling_rate)
    
    def compute_frequency_features(self, sequences):
        """
        æŒ‰æ‰¹æ¬¡è®¡ç®—é¢‘åŸŸç‰¹å¾ï¼Œé¿å…å†…å­˜æº¢å‡º
        sequences: [n_samples, seq_len, n_features]
        """
        n_samples, seq_len, n_features = sequences.shape
        n_freq_features = self.config.freq_bands * n_features
        
        # åˆ›å»ºç©ºæ•°ç»„ï¼ˆåªåˆ†é…è¾“å‡ºç©ºé—´ï¼‰
        freq_features = np.zeros((n_samples, seq_len, n_freq_features), dtype=np.float32)
        
        # åˆ†æ‰¹æ¬¡å¤„ç†ï¼ˆæ¯æ¬¡å¤„ç†5000ä¸ªæ ·æœ¬ï¼‰
        batch_size = 5000
        for start_idx in range(0, n_samples, batch_size):
            end_idx = min(start_idx + batch_size, n_samples)
            batch_sequences = sequences[start_idx:end_idx]
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            self._process_batch(batch_sequences, freq_features, start_idx, end_idx)
            
            print(f"  å¤„ç†é¢‘åŸŸç‰¹å¾: {end_idx}/{n_samples} samples")
        
        return freq_features
    
    def _process_batch(self, batch_sequences, freq_features, start_idx, end_idx):
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„é¢‘åŸŸç‰¹å¾"""
        batch_size, seq_len, n_features = batch_sequences.shape
        
        for feature_idx in range(n_features):
            # å¯¹æ¯ä¸ªç‰¹å¾è®¡ç®—FFT
            feature_data = batch_sequences[:, :, feature_idx]  # [batch_size, seq_len]
            fft_result = np.fft.rfft(feature_data, axis=1)  # [batch_size, fft_coefficients]
            fft_magnitude = np.abs(fft_result)  # [batch_size, fft_coefficients]
            
            # ä¸ºæ¯ä¸ªé¢‘ç‡å¸¦è®¡ç®—èƒ½é‡
            for band_idx in range(self.config.freq_bands):
                low_freq = self.freq_bands[band_idx]
                high_freq = self.freq_bands[band_idx+1]
                
                # æ‰¾åˆ°å¯¹åº”é¢‘ç‡èŒƒå›´çš„ç´¢å¼•
                band_indices = np.where((self.fft_freqs >= low_freq) & 
                                       (self.fft_freqs < high_freq))[0]
                
                if len(band_indices) > 0:
                    # è®¡ç®—è¯¥é¢‘å¸¦çš„å¹³å‡èƒ½é‡ - [batch_size, len(band_indices)] -> [batch_size]
                    band_energy = np.mean(fft_magnitude[:, band_indices], axis=1)
                    
                    # å°†èƒ½é‡åˆ†é…åˆ°æ‰€æœ‰æ—¶é—´æ­¥
                    start_col = band_idx * n_features + feature_idx
                    # band_energy: [batch_size] -> [batch_size, 1]
                    # ç„¶åå¹¿æ’­åˆ° [batch_size, seq_len]
                    freq_features[start_idx:end_idx, :, start_col] = \
                        np.broadcast_to(band_energy[:, np.newaxis], 
                                       (batch_size, seq_len))

# ==================== æ•°æ®å¤„ç†å™¨ ====================
def prepare_data(config):
    print("ğŸ”„ åŠ è½½å’Œå¤„ç†æ•°æ®...")
    df = pd.read_csv(config.data_path)
    
    # é€‰æ‹©æ­£å¸¸æœºå™¨çš„æ•°æ®ï¼ˆæ— æ•…éšœï¼‰
    normal_df = df[df['fault_label'] == 0].copy()
    print(f"   æ­£å¸¸æœºå™¨æ•°æ®: {len(normal_df)} / {len(df)}")
    
    # é™åˆ¶æ€»æ ·æœ¬æ•°ä»¥èŠ‚çœå†…å­˜
    max_total_samples = 100000
    if len(normal_df) > max_total_samples:
        normal_df = normal_df.sample(n=max_total_samples, random_state=42)
        print(f"   é™åˆ¶æ•°æ®é‡: {len(normal_df)}")
    
    # æå–ç‰¹å¾å’Œç›®æ ‡
    features = normal_df[config.feature_cols].values
    targets = normal_df[config.target_cols].values
    
    # æ ‡å‡†åŒ–
    feature_mean = features.mean(axis=0)
    feature_std = features.std(axis=0)
    feature_std[feature_std < 1e-8] = 1.0
    
    target_mean = targets.mean(axis=0)
    target_std = targets.std(axis=0)
    target_std[target_std < 1e-8] = 1.0
    
    features_norm = (features - feature_mean) / feature_std
    targets_norm = (targets - target_mean) / target_std
    
    # åˆ›å»ºåºåˆ—æ ·æœ¬
    sequences = []
    target_values = []
    
    machine_ids = normal_df['machine_id'].unique()
    
    for mid in machine_ids:
        machine_data = normal_df[normal_df['machine_id'] == mid]
        if len(machine_data) < config.seq_len + config.pred_len:
            continue
        
        machine_features = features_norm[normal_df['machine_id'] == mid]
        machine_targets = targets_norm[normal_df['machine_id'] == mid]
        
        # é™åˆ¶æ¯å°æœºå™¨ç”Ÿæˆçš„æ ·æœ¬æ•°é‡
        max_samples_per_machine = 1000
        n_windows = min(len(machine_data) - config.seq_len - config.pred_len + 1, 
                        max_samples_per_machine)
        
        for i in range(n_windows):
            seq = machine_features[i:i+config.seq_len]
            target_idx = i + config.seq_len + config.pred_len - 1
            target_val = machine_targets[target_idx]
            
            sequences.append(seq)
            target_values.append(target_val)
    
    sequences = np.array(sequences)
    target_values = np.array(target_values)
    
    # æœ€ç»ˆé™åˆ¶æ€»æ ·æœ¬æ•°
    max_final_samples = 50000
    if len(sequences) > max_final_samples:
        idx = np.random.choice(len(sequences), max_final_samples, replace=False)
        sequences = sequences[idx]
        target_values = target_values[idx]
    
    # é¢„è®¡ç®—é¢‘åŸŸç‰¹å¾
    print("ğŸ“Š è®¡ç®—é¢‘åŸŸç‰¹å¾...")
    freq_processor = FrequencyFeatureProcessor(config, feature_mean, feature_std)
    freq_features = freq_processor.compute_frequency_features(sequences)
    
    # åˆå¹¶æ—¶åŸŸå’Œé¢‘åŸŸç‰¹å¾
    combined_sequences = np.concatenate([sequences, freq_features], axis=2)
    print(f"   åˆå¹¶åç‰¹å¾ç»´åº¦: {combined_sequences.shape[2]} (æ—¶åŸŸ: {sequences.shape[2]}, é¢‘åŸŸ: {freq_features.shape[2]})")
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_seq, val_seq, train_targets, val_targets = train_test_split(
        combined_sequences, target_values, test_size=0.2, random_state=42
    )
    
    print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(sequences)}")
    print(f"   è®­ç»ƒé›†: {len(train_seq)}, éªŒè¯é›†: {len(val_seq)}")
    
    # ä¿å­˜æ ‡å‡†åŒ–å‚æ•°
    normalization_params = {
        'feature_mean': feature_mean,
        'feature_std': feature_std,
        'target_mean': target_mean,
        'target_std': target_std,
        'feature_cols': config.feature_cols,
        'target_cols': config.target_cols,
        'freq_bands': config.freq_bands,
        'sampling_rate': config.sampling_rate
    }
    
    with open(os.path.join(config.checkpoint_dir, 'normalization_params.pkl'), 'wb') as f:
        pickle.dump(normalization_params, f)
    
    return (train_seq, train_targets), (val_seq, val_targets), normalization_params

# ==================== è®­ç»ƒå‡½æ•° ====================
def train_model(config):
    print("=" * 80)
    print("ğŸš€ è®­ç»ƒå¢å¼ºç‰ˆç‰©ç†é¢„æµ‹æ¨¡å‹")
    print("=" * 80)
    
    # å‡†å¤‡æ•°æ®
    (train_seq, train_targets), (val_seq, val_targets), norm_params = prepare_data(config)
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = PhysicalPredictionDataset(train_seq, train_targets)
    val_dataset = PhysicalPredictionDataset(val_seq, val_targets)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = EnhancedPhysicalPredictor(config)
    print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ | å‚æ•°é‡: {sum(p.numel() for p in model.parameters())}")
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¤šGPU
    if len(config.gpu_ids) > 1:
        print(f"âœ… ä½¿ç”¨å¤šGPUè®­ç»ƒ: {config.gpu_ids}")
        model = nn.DataParallel(model, device_ids=config.gpu_ids)
        model = model.to(config.device)
    else:
        model = model.to(config.device)
    
    # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.lr,
        weight_decay=1e-5
    )
    
    warmup_scheduler = LinearLR(
        optimizer,
        start_factor=0.1,
        end_factor=1.0,
        total_iters=config.warmup_epochs * len(train_loader)
    )
    
    cosine_scheduler = CosineAnnealingLR(
        optimizer,
        T_max=(config.epochs - config.warmup_epochs) * len(train_loader),
        eta_min=1e-6
    )
    
    scheduler = SequentialLR(
        optimizer,
        schedulers=[warmup_scheduler, cosine_scheduler],
        milestones=[config.warmup_epochs * len(train_loader)]
    )
    
    # æŸå¤±å‡½æ•°
    criterion = nn.MSELoss()
    scaler = GradScaler('cuda')
    
    # ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    start_epoch = 0
    best_val_loss = float('inf')
    train_losses = []
    val_losses = []
    
    if config.resume_from and os.path.exists(config.resume_from):
        print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {config.resume_from}")
        checkpoint = torch.load(config.resume_from)
        if isinstance(model, nn.DataParallel):
            model.module.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch']
        best_val_loss = checkpoint['best_val_loss']
        train_losses = checkpoint.get('train_losses', [])
        val_losses = checkpoint.get('val_losses', [])
        print(f"âœ… æ¢å¤è®­ç»ƒæˆåŠŸ | ä»ç¬¬ {start_epoch} ä¸ªepochå¼€å§‹")
    
    print("\nğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    print("-" * 80)
    
    for epoch in range(start_epoch, config.epochs):
        epoch_start = time.time()
        model.train()
        total_loss = 0
        total_physics_loss = 0
        total_freq_loss = 0
        
        for batch_idx, (seq, target) in enumerate(train_loader):
            seq, target = seq.to(config.device), target.to(config.device)
            
            with autocast('cuda'):
                pred = model(seq)
                data_loss = criterion(pred, target)
                
                # ç‰©ç†çº¦æŸæŸå¤± - æ£€æŸ¥æ˜¯å¦ä½¿ç”¨DataParallel
                if isinstance(model, nn.DataParallel):
                    physics_loss = model.module.physics_loss(pred, target)
                    freq_loss = model.module.frequency_loss(pred, target)
                else:
                    physics_loss = model.physics_loss(pred, target)
                    freq_loss = model.frequency_loss(pred, target)
                
                # æ€»æŸå¤±
                total_batch_loss = data_loss + config.lambda_physics * physics_loss + config.lambda_freq * freq_loss
            
            # åå‘ä¼ æ’­
            scaler.scale(total_batch_loss).backward()
            
            if (batch_idx + 1) % config.gradient_accumulation_steps == 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
                scheduler.step()
            
            total_loss += total_batch_loss.item()
            total_physics_loss += physics_loss.item()
            total_freq_loss += freq_loss.item()
        
        avg_train_loss = total_loss / len(train_loader)
        avg_physics_loss = total_physics_loss / len(train_loader)
        avg_freq_loss = total_freq_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # éªŒè¯
        model.eval()
        val_loss = 0
        val_physics_loss = 0
        
        with torch.no_grad():
            for seq, target in val_loader:
                seq, target = seq.to(config.device), target.to(config.device)
                pred = model(seq)
                loss = criterion(pred, target)
                
                # ç‰©ç†çº¦æŸæŸå¤± - æ£€æŸ¥æ˜¯å¦ä½¿ç”¨DataParallel
                if isinstance(model, nn.DataParallel):
                    physics_loss = model.module.physics_loss(pred, target)
                else:
                    physics_loss = model.physics_loss(pred, target)
                
                val_loss += loss.item()
                val_physics_loss += physics_loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        avg_val_physics_loss = val_physics_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        
        epoch_time = time.time() - epoch_start
        
        # è®¡ç®—å‰©ä½™æ—¶é—´
        elapsed_time = time.time() - epoch_start
        remaining_epochs = config.epochs - epoch - 1
        remaining_time = elapsed_time * remaining_epochs
        remaining_time_str = str(timedelta(seconds=int(remaining_time)))
        
        print(f"âœ… Epoch {epoch+1:2d}/{config.epochs} | "
              f"Train Loss: {avg_train_loss:.6f} (Physics: {avg_physics_loss:.6f}, Freq: {avg_freq_loss:.6f}) | "
              f"Val Loss: {avg_val_loss:.6f} (Physics: {avg_val_physics_loss:.6f}) | "
              f"Time: {epoch_time:.2f}s | "
              f"å‰©ä½™æ—¶é—´: {remaining_time_str}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            checkpoint_data = {
                'epoch': epoch + 1,
                'model_state_dict': model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'best_val_loss': best_val_loss,
                'train_losses': train_losses,
                'val_losses': val_losses,
                'config': config.__dict__
            }
            torch.save(checkpoint_data, os.path.join(config.checkpoint_dir, 'best_physical_predictor.pth'))
            print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {best_val_loss:.6f})")
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % 5 == 0:
            checkpoint_data = {
                'epoch': epoch + 1,
                'model_state_dict': model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'best_val_loss': best_val_loss,
                'train_losses': train_losses,
                'val_losses': val_losses,
                'config': config.__dict__
            }
            torch.save(checkpoint_data, os.path.join(config.checkpoint_dir, f'checkpoint_epoch{epoch+1}.pth'))
            print(f"   ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: epoch {epoch+1}")
        
        # æ¯10ä¸ªepochç”Ÿæˆä¸€æ¬¡å¯è§†åŒ–
        if (epoch + 1) % 10 == 0:
            visualize_training_progress(model, val_loader, config, epoch + 1)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='è®­ç»ƒæŸå¤±')
    plt.plot(val_losses, label='éªŒè¯æŸå¤±')
    plt.xlabel('Epoch')
    plt.ylabel('æŸå¤±')
    plt.title('å¢å¼ºç‰ˆç‰©ç†é¢„æµ‹æ¨¡å‹è®­ç»ƒè¿‡ç¨‹')
    plt.legend()
    plt.grid(True)
    plt.savefig(os.path.join(config.checkpoint_dir, 'training_curve_enhanced.png'))
    
    print("\n" + "=" * 80)
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print("=" * 80)

def visualize_training_progress(model, val_loader, config, epoch):
    """å¯è§†åŒ–éªŒè¯é›†ä¸Šçš„é¢„æµ‹ç»“æœ"""
    model.eval()
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for seq, target in val_loader:
            seq = seq.to(config.device)
            pred = model(seq)
            all_preds.append(pred.cpu().numpy())
            all_targets.append(target.cpu().numpy())
            
            if len(all_preds) * config.batch_size > 1000:  # é™åˆ¶æ ·æœ¬æ•°é‡
                break
    
    preds = np.concatenate(all_preds, axis=0)
    targets = np.concatenate(all_targets, axis=0)
    
    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle(f'å¢å¼ºç‰ˆç‰©ç†é¢„æµ‹æ¨¡å‹ - Epoch {epoch}', fontsize=16)
    
    target_names = ['Vib X', 'Vib Y', 'Temp', 'Motor X', 'Motor Y']
    
    for i in range(min(5, len(target_names))):
        ax = axes[i//3, i%3]
        ax.scatter(targets[:200, i], preds[:200, i], alpha=0.6, s=10)
        ax.plot([targets[:200, i].min(), targets[:200, i].max()], 
                [targets[:200, i].min(), targets[:200, i].max()], 'r--')
        ax.set_xlabel('çœŸå®å€¼')
        ax.set_ylabel('é¢„æµ‹å€¼')
        ax.set_title(target_names[i])
        ax.grid(True)
    
    # ç‰©ç†ä¸€è‡´æ€§æ£€æŸ¥
    ax = axes[1, 2]
    
    # è®¡ç®—æŒ¯åŠ¨å’Œç”µæœºç”µæµçš„ç›¸å…³æ€§
    vib_magnitude = np.sqrt(preds[:, 0]**2 + preds[:, 1]**2)
    motor_magnitude = np.sqrt(preds[:, 3]**2 + preds[:, 4]**2)
    
    ax.scatter(vib_magnitude[:200], motor_magnitude[:200], alpha=0.6, s=10)
    ax.set_xlabel('æŒ¯åŠ¨å¹…åº¦')
    ax.set_ylabel('ç”µæœºç”µæµå¹…åº¦')
    ax.set_title('ç‰©ç†ä¸€è‡´æ€§: æŒ¯åŠ¨-ç”µæµå…³ç³»')
    ax.grid(True)
    
    plt.tight_layout()
    plt.savefig(os.path.join(config.checkpoint_dir, f'validation_results_epoch{epoch}.png'))
    plt.close()

# ==================== ä¸»å‡½æ•° ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='è®­ç»ƒå¢å¼ºç‰ˆç‰©ç†é¢„æµ‹æ¨¡å‹')
    parser.add_argument('--resume', type=str, default=None, help='ä»æŒ‡å®šè·¯å¾„æ¢å¤è®­ç»ƒ')
    parser.add_argument('--gpu_ids', type=str, default='0,1', help='GPU IDs (ä¾‹å¦‚: "0,1,2,3")')
    args = parser.parse_args()
    
    gpu_ids = [int(id) for id in args.gpu_ids.split(',')]
    config = Config(resume_from=args.resume, gpu_ids=gpu_ids)
    
    train_model(config)