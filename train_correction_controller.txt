# train_correction_controller.py
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
import os
import time
import pickle
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from torch.optim import AdamW
from torch.amp import autocast, GradScaler
import argparse
from datetime import datetime, timedelta

# é…ç½®matplotlibæ”¯æŒä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'Liberation Sans']
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# ==================== é…ç½®å‚æ•° ====================
class Config:
    def __init__(self, resume_from=None, gpu_ids=[0]):
        self.data_path = 'printer_dataset_correction/printer_gear_correction_dataset.csv'
        self.pred_model_path = './checkpoints_physical_predictor/best_physical_predictor.pth'
        self.batch_size = 1024
        self.lr = 3e-4
        self.epochs = 30
        self.gpu_ids = gpu_ids
        self.resume_from = resume_from  # æ·»åŠ ç»§ç»­è®­ç»ƒçš„è·¯å¾„
        if len(gpu_ids) > 1:
            self.device = f'cuda:{gpu_ids[0]}'  # ä¸»GPU
        else:
            self.device = f'cuda:{gpu_ids[0]}' if torch.cuda.is_available() else 'cpu'
        self.checkpoint_dir = './checkpoints_correction_controller'
        self.max_samples = 50000
        self.seq_len = 50  # çŸ­åºåˆ—ï¼Œç”¨äºå®æ—¶æ§åˆ¶
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # ç‰¹å¾åˆ—
        self.feature_cols = [
            'ctrl_T_target', 'ctrl_speed_set', 'ctrl_pos_x', 'ctrl_pos_y', 'ctrl_pos_z',
            'temperature_C', 'vibration_disp_x_m', 'vibration_disp_y_m',
            'vibration_vel_x_m_s', 'vibration_vel_y_m_s',
            'motor_current_x_A', 'motor_current_y_A',
            'pressure_bar'
        ]
        
        # çŸ«æ­£ç›®æ ‡åˆ—
        self.correction_cols = [
            'correction_x_mm', 'correction_y_mm', 'correction_temp_C'
        ]
        
        self.input_dim = len(self.feature_cols)
        self.output_dim = len(self.correction_cols)

# ==================== çŸ«æ­£æ§åˆ¶å™¨æ¨¡å‹ ====================
class CorrectionController(nn.Module):
    def __init__(self, config):
        super(CorrectionController, self).__init__()
        self.config = config
        
        # ä¸¤å±‚MLP
        self.net = nn.Sequential(
            nn.Linear(config.input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, config.output_dim)
        )
    
    def forward(self, x):
        # x: [batch, input_dim]
        return self.net(x)

# ==================== æ•°æ®é›†ç±» ====================
class CorrectionDataset(Dataset):
    def __init__(self, features, corrections):
        self.features = torch.tensor(features, dtype=torch.float32)
        self.corrections = torch.tensor(corrections, dtype=torch.float32)
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return self.features[idx], self.corrections[idx]

# ==================== æ•°æ®å¤„ç†å™¨ ====================
def prepare_correction_data(config):
    print("ğŸ”„ åŠ è½½çŸ«æ­£æ•°æ®...")
    df = pd.read_csv(config.data_path)
    
    # é€‰æ‹©æ­£å¸¸æœºå™¨ä½†æœ‰çŸ«æ­£ä¿¡å·çš„æ•°æ®
    normal_df = df[df['fault_label'] == 0].copy()
    
    # æˆ‘ä»¬åªå…³å¿ƒæŒ¯åŠ¨è¾ƒå¤§çš„åŒºåŸŸï¼ˆéœ€è¦çŸ«æ­£çš„åœ°æ–¹ï¼‰
    normal_df = normal_df[normal_df['vibration_disp_x_m'].abs() + normal_df['vibration_disp_y_m'].abs() > 0.0005]
    
    print(f"   æœ‰æ•ˆçŸ«æ­£æ ·æœ¬: {len(normal_df)}")
    
    # é‡‡æ ·
    if len(normal_df) > config.max_samples:
        normal_df = normal_df.sample(n=config.max_samples, random_state=42)
        print(f"   é‡‡æ ·åæ ·æœ¬æ•°: {len(normal_df)}")
    
    # æå–ç‰¹å¾å’ŒçŸ«æ­£ç›®æ ‡
    features = normal_df[config.feature_cols].values
    corrections = normal_df[config.correction_cols].values
    
    # ä»ç‰©ç†é¢„æµ‹æ¨¡å‹åŠ è½½æ ‡å‡†åŒ–å‚æ•°
    norm_params_path = './checkpoints_physical_predictor/normalization_params.pkl'
    if os.path.exists(norm_params_path):
        with open(norm_params_path, 'rb') as f:
            norm_params = pickle.load(f)
        
        feature_mean = norm_params['feature_mean']
        feature_std = norm_params['feature_std']
        features_norm = (features - feature_mean) / feature_std
        print("âœ… ä½¿ç”¨ç‰©ç†é¢„æµ‹æ¨¡å‹çš„æ ‡å‡†åŒ–å‚æ•°")
    else:
        # å¦‚æœæ²¡æœ‰ï¼Œè‡ªå·±è®¡ç®—
        feature_mean = features.mean(axis=0)
        feature_std = features.std(axis=0)
        feature_std[feature_std < 1e-8] = 1.0
        features_norm = (features - feature_mean) / feature_std
    
    # ç›®æ ‡æ ‡å‡†åŒ–
    correction_mean = corrections.mean(axis=0)
    correction_std = corrections.std(axis=0)
    correction_std[correction_std < 1e-8] = 1.0
    corrections_norm = (corrections - correction_mean) / correction_std
    
    # ä¿å­˜çŸ«æ­£æ ‡å‡†åŒ–å‚æ•°
    correction_params = {
        'correction_mean': correction_mean,
        'correction_std': correction_std,
        'correction_cols': config.correction_cols
    }
    
    with open(os.path.join(config.checkpoint_dir, 'correction_params.pkl'), 'wb') as f:
        pickle.dump(correction_params, f)
    
    # åˆ†å‰²æ•°æ®é›†
    train_feat, val_feat, train_corr, val_corr = train_test_split(
        features_norm, corrections_norm, test_size=0.2, random_state=42
    )
    
    print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(features_norm)}")
    print(f"   è®­ç»ƒé›†: {len(train_feat)}, éªŒè¯é›†: {len(val_feat)}")
    
    return (train_feat, train_corr), (val_feat, val_corr), correction_params

# ==================== è®­ç»ƒå‡½æ•° ====================
def train_correction_controller(config):
    print("=" * 80)
    print("ğŸš€ è®­ç»ƒçŸ«æ­£æ§åˆ¶å™¨")
    print("=" * 80)
    
    # å‡†å¤‡æ•°æ®
    (train_feat, train_corr), (val_feat, val_corr), corr_params = prepare_correction_data(config)
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = CorrectionDataset(train_feat, train_corr)
    val_dataset = CorrectionDataset(val_feat, val_corr)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = CorrectionController(config)
    print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ | å‚æ•°é‡: {sum(p.numel() for p in model.parameters())}")
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¤šGPU
    if len(config.gpu_ids) > 1:
        print(f"âœ… ä½¿ç”¨å¤šGPUè®­ç»ƒ: {config.gpu_ids}")
        model = nn.DataParallel(model, device_ids=config.gpu_ids)
        model = model.to(config.device)
    else:
        model = model.to(config.device)
    
    # ä¼˜åŒ–å™¨
    optimizer = AdamW(model.parameters(), lr=config.lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3
    )
    
    # æŸå¤±å‡½æ•°
    criterion = nn.MSELoss()
    scaler = GradScaler('cuda')
    
    # ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    start_epoch = 0
    best_val_loss = float('inf')
    train_losses = []
    val_losses = []
    
    if config.resume_from and os.path.exists(config.resume_from):
        print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {config.resume_from}")
        checkpoint = torch.load(config.resume_from)
        if isinstance(model, nn.DataParallel):
            model.module.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch']
        best_val_loss = checkpoint['best_val_loss']
        train_losses = checkpoint.get('train_losses', [])
        val_losses = checkpoint.get('val_losses', [])
        print(f"âœ… æ¢å¤è®­ç»ƒæˆåŠŸ | ä»ç¬¬ {start_epoch} ä¸ªepochå¼€å§‹")

    print("\nğŸ”¥ å¼€å§‹è®­ç»ƒçŸ«æ­£æ§åˆ¶å™¨...")
    print("-" * 80)
    
    for epoch in range(start_epoch, config.epochs):
        epoch_start = time.time()
        model.train()
        total_loss = 0
        
        for batch_idx, (feat, corr) in enumerate(train_loader):
            feat, corr = feat.to(config.device), corr.to(config.device)
            
            with autocast('cuda'):
                pred = model(feat)
                loss = criterion(pred, corr)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            
            total_loss += loss.item()
        
        avg_train_loss = total_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # éªŒè¯
        model.eval()
        val_loss = 0
        
        with torch.no_grad():
            for feat, corr in val_loader:
                feat, corr = feat.to(config.device), corr.to(config.device)
                pred = model(feat)
                loss = criterion(pred, corr)
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        
        scheduler.step(avg_val_loss)
        epoch_time = time.time() - epoch_start
        
        # è®¡ç®—å‰©ä½™æ—¶é—´
        elapsed_time = time.time() - epoch_start
        remaining_epochs = config.epochs - epoch - 1
        remaining_time = elapsed_time * remaining_epochs
        remaining_time_str = str(timedelta(seconds=int(remaining_time)))
        
        print(f"âœ… Epoch {epoch+1:2d}/{config.epochs} | "
              f"Train Loss: {avg_train_loss:.6f} | "
              f"Val Loss: {avg_val_loss:.6f} | "
              f"Time: {epoch_time:.2f}s | "
              f"å‰©ä½™æ—¶é—´: {remaining_time_str}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            checkpoint_data = {
                'epoch': epoch + 1,
                'model_state_dict': model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'best_val_loss': best_val_loss,
                'train_losses': train_losses,
                'val_losses': val_losses,
                'config': config.__dict__
            }
            torch.save(checkpoint_data, os.path.join(config.checkpoint_dir, 'best_correction_controller.pth'))
            print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³çŸ«æ­£æ§åˆ¶å™¨ (éªŒè¯æŸå¤±: {best_val_loss:.6f})")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='è®­ç»ƒæŸå¤±')
    plt.plot(val_losses, label='éªŒè¯æŸå¤±')
    plt.xlabel('Epoch')
    plt.ylabel('æŸå¤±')
    plt.title('çŸ«æ­£æ§åˆ¶å™¨è®­ç»ƒè¿‡ç¨‹')
    plt.legend()
    plt.grid(True)
    plt.savefig(os.path.join(config.checkpoint_dir, 'correction_training_curve.png'), 
                bbox_inches='tight', dpi=300, facecolor='white')
    
    print("\n" + "=" * 80)
    print(f"ğŸ‰ çŸ«æ­£æ§åˆ¶å™¨è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print("=" * 80)

# ==================== ä¸»å‡½æ•° ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='è®­ç»ƒçŸ«æ­£æ§åˆ¶å™¨')
    parser.add_argument('--resume', type=str, default=None, help='ä»æŒ‡å®šè·¯å¾„æ¢å¤è®­ç»ƒ')
    parser.add_argument('--gpu_ids', type=str, default='0,1', help='GPU IDs (ä¾‹å¦‚: "0,1,2,3")')
    args = parser.parse_args()
    
    gpu_ids = [int(id) for id in args.gpu_ids.split(',')]
    config = Config(resume_from=args.resume, gpu_ids=gpu_ids)
    train_correction_controller(config)